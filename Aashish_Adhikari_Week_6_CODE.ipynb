{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDmHfGwyvg7Ik5Tnk/QQXp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anonymousboy67/Document-Similarity-Analysis/blob/main/Aashish_Adhikari_Week_6_CODE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Zip files**"
      ],
      "metadata": {
        "id": "Vu-ggOBUM84t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "G5sk_KwBLFVr",
        "outputId": "fcf1d40d-c19d-467b-da9e-0fd32d107f86"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2fbe639b-8e0f-42c8-90cf-904abc6e4f72\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2fbe639b-8e0f-42c8-90cf-904abc6e4f72\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving archive.zip to archive.zip\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded=files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extracting the zip file**"
      ],
      "metadata": {
        "id": "YPmZYhuEM4If"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('archive.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('cisi_data')\n"
      ],
      "metadata": {
        "id": "elM8rRyvLRW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lisiting files in zip**"
      ],
      "metadata": {
        "id": "VfK4vvZYNDm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Files in dataset:\")\n",
        "for file in os.listdir('cisi_data'):\n",
        "    print(f\"  - {file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXP_y1kaMh4M",
        "outputId": "08cffb83-1f42-495f-ba74-e5fcedadb331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in dataset:\n",
            "  - CISI.REL\n",
            "  - CISI.QRY\n",
            "  - CISI.ALL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FIRST 30 LINES OF CISI.ALL (documents)**"
      ],
      "metadata": {
        "id": "hdTXH011NbNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('cisi_data/CISI.ALL', 'r', encoding='utf-8', errors='ignore') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i < 30:  # First 30 lines\n",
        "            print(line.rstrip())\n",
        "        else:\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmDqpp64MuSU",
        "outputId": "65c8c9e7-e9e2-4c11-f1d2-ab0b1798a177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".I 1\n",
            ".T\n",
            "18 Editions of the Dewey Decimal Classifications\n",
            ".A\n",
            "Comaromi, J.P.\n",
            ".W\n",
            "   The present study is a history of the DEWEY Decimal\n",
            "Classification.  The first edition of the DDC was published\n",
            "in 1876, the eighteenth edition in 1971, and future editions\n",
            "will continue to appear as needed.  In spite of the DDC's\n",
            "long and healthy life, however, its full story has never\n",
            "been told.  There have been biographies of Dewey\n",
            "that briefly describe his system, but this is the first\n",
            "attempt to provide a detailed history of the work that\n",
            "more than any other has spurred the growth of\n",
            "librarianship in this country and abroad.\n",
            ".X\n",
            "1\t5\t1\n",
            "92\t1\t1\n",
            "262\t1\t1\n",
            "556\t1\t1\n",
            "1004\t1\t1\n",
            "1024\t1\t1\n",
            "1024\t1\t1\n",
            ".I 2\n",
            ".T\n",
            "Use Made of Technical Libraries\n",
            ".A\n",
            "Slater, M.\n",
            ".W\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FIRST 20 LINES OF CISI.QRY (Queries)**"
      ],
      "metadata": {
        "id": "RcepPlrMOCEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('cisi_data/CISI.QRY', 'r', encoding='utf-8', errors='ignore') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i < 20:\n",
        "            print(line.rstrip())\n",
        "        else:\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDOrSj2lN7rE",
        "outputId": "458f5c72-e65a-43d3-aaaf-0b00aaa06db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".I 1\n",
            ".W\n",
            "What problems and concerns are there in making up descriptive titles?\n",
            "What difficulties are involved in automatically retrieving articles from\n",
            "approximate titles?\n",
            "What is the usual relevance of the content of articles to their titles?\n",
            ".I 2\n",
            ".W\n",
            "How can actually pertinent data, as opposed to references or entire articles\n",
            "themselves, be retrieved automatically in response to information requests?\n",
            ".I 3\n",
            ".W\n",
            "What is information science?  Give definitions where possible.\n",
            ".I 4\n",
            ".W\n",
            "Image recognition and any other methods of automatically\n",
            "transforming printed text into computer-ready form.\n",
            ".I 5\n",
            ".W\n",
            "What special training will ordinary researchers and businessmen need for proper\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FIRST 20 LINES OF CISI.REL (Relevance Judgments)**"
      ],
      "metadata": {
        "id": "GHBNaaRJOhAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('cisi_data/CISI.REL', 'r', encoding='utf-8', errors='ignore') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i < 20:\n",
        "            print(line.rstrip())\n",
        "        else:\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fr_NtRlHOdJ_",
        "outputId": "c5257b9e-f259-430d-be3f-8c32662ae258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     1     28\t0\t0.000000\n",
            "     1     35\t0\t0.000000\n",
            "     1     38\t0\t0.000000\n",
            "     1     42\t0\t0.000000\n",
            "     1     43\t0\t0.000000\n",
            "     1     52\t0\t0.000000\n",
            "     1     65\t0\t0.000000\n",
            "     1     76\t0\t0.000000\n",
            "     1     86\t0\t0.000000\n",
            "     1    150\t0\t0.000000\n",
            "     1    189\t0\t0.000000\n",
            "     1    192\t0\t0.000000\n",
            "     1    193\t0\t0.000000\n",
            "     1    195\t0\t0.000000\n",
            "     1    215\t0\t0.000000\n",
            "     1    269\t0\t0.000000\n",
            "     1    291\t0\t0.000000\n",
            "     1    320\t0\t0.000000\n",
            "     1    429\t0\t0.000000\n",
            "     1    465\t0\t0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parsing CISI Dataset**"
      ],
      "metadata": {
        "id": "KaTi0tK5P1rY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def parse_documents(file_path):\n",
        "\n",
        "    documents = {}\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            content = f.read()\n",
        "\n",
        "\n",
        "        doc_parts = re.split(r'\\.I\\s+(\\d+)', content)[1:]\n",
        "\n",
        "\n",
        "        for i in range(0, len(doc_parts), 2):\n",
        "            doc_id = int(doc_parts[i])\n",
        "            doc_content = doc_parts[i + 1]\n",
        "\n",
        "\n",
        "            title_match = re.search(r'\\.T\\s+(.*?)(?=\\.[AWBX]|\\Z)', doc_content, re.DOTALL)\n",
        "            title = title_match.group(1).strip() if title_match else \"\"\n",
        "\n",
        "\n",
        "            author_match = re.search(r'\\.A\\s+(.*?)(?=\\.[TWBX]|\\Z)', doc_content, re.DOTALL)\n",
        "            author = author_match.group(1).strip() if author_match else \"\"\n",
        "\n",
        "\n",
        "            content_match = re.search(r'\\.W\\s+(.*?)(?=\\.I|\\Z)', doc_content, re.DOTALL)\n",
        "            main_content = content_match.group(1).strip() if content_match else \"\"\n",
        "\n",
        "\n",
        "            full_content = title + \" \" + main_content\n",
        "\n",
        "\n",
        "            documents[doc_id] = {\n",
        "                'id': doc_id,\n",
        "                'title': title,\n",
        "                'author': author,\n",
        "                'content': full_content.strip()\n",
        "            }\n",
        "\n",
        "        print(f\"Successfully parsed {len(documents)} documents from CISI.ALL\")\n",
        "        return documents\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\" Error: File '{file_path}' not found!\")\n",
        "        return {}\n",
        "    except Exception as e:\n",
        "        print(f\" Error parsing documents: {e}\")\n",
        "        return {}"
      ],
      "metadata": {
        "id": "2Q84q-agO7vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parse Queries (CISI.QRY)**"
      ],
      "metadata": {
        "id": "so4Z9WiQRpVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_queries(file_path):\n",
        "\n",
        "    queries = {}\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            content = f.read()\n",
        "\n",
        "\n",
        "        query_parts = re.split(r'\\.I\\s+(\\d+)', content)[1:]\n",
        "\n",
        "\n",
        "        for i in range(0, len(query_parts), 2):\n",
        "            query_id = int(query_parts[i])\n",
        "            query_content = query_parts[i + 1]\n",
        "\n",
        "\n",
        "            text_match = re.search(r'\\.W\\s+(.*?)(?=\\.I|\\Z)', query_content, re.DOTALL)\n",
        "            query_text = text_match.group(1).strip() if text_match else \"\"\n",
        "\n",
        "\n",
        "            queries[query_id] = {\n",
        "                'id': query_id,\n",
        "                'text': query_text\n",
        "            }\n",
        "\n",
        "        print(f\" Successfully parsed {len(queries)} queries from CISI.QRY\")\n",
        "        return queries\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found!\")\n",
        "        return {}\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing queries: {e}\")\n",
        "        return {}\n",
        "\n"
      ],
      "metadata": {
        "id": "PF2Z04jdRs43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parse Relevance Judgments (CISI.REL)**"
      ],
      "metadata": {
        "id": "aKwwEoAlR2-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_relevance(file_path):\n",
        "\n",
        "    relevance = defaultdict(list)\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "\n",
        "\n",
        "                parts = line.split()\n",
        "                if len(parts) >= 2:\n",
        "                    query_id = int(parts[0])\n",
        "                    doc_id = int(parts[1])\n",
        "\n",
        "\n",
        "                    relevance[query_id].append(doc_id)\n",
        "\n",
        "        relevance = dict(relevance)\n",
        "\n",
        "        print(f\"Successfully parsed relevance judgments for {len(relevance)} queries from CISI.REL\")\n",
        "        return relevance\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found!\")\n",
        "        return {}\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing relevance judgments: {e}\")\n",
        "        return {}\n",
        "\n"
      ],
      "metadata": {
        "id": "rfJw5-o4R73M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cisi_dataset(base_path='cisi_data'):\n",
        "\n",
        "    import os\n",
        "    print(\"LOADING CISI DATASET\")\n",
        "\n",
        "\n",
        "\n",
        "    docs_path = os.path.join(base_path, 'CISI.ALL')\n",
        "    queries_path = os.path.join(base_path, 'CISI.QRY')\n",
        "    relevance_path = os.path.join(base_path, 'CISI.REL')\n",
        "\n",
        "\n",
        "    documents = parse_documents(docs_path)\n",
        "    queries = parse_queries(queries_path)\n",
        "    relevance = parse_relevance(relevance_path)\n",
        "\n",
        "\n",
        "    print(\"DATASET SUMMARY\")\n",
        "\n",
        "    print(f\"Total Documents: {len(documents)}\")\n",
        "    print(f\"Total Queries: {len(queries)}\")\n",
        "    print(f\"Queries with Relevance Judgments: {len(relevance)}\")\n",
        "\n",
        "\n",
        "    return documents, queries, relevance\n"
      ],
      "metadata": {
        "id": "6AOCB2eYSEoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_samples(documents, queries, relevance, num_samples=3):\n",
        "\n",
        "\n",
        "    print(\"SAMPLE DOCUMENTS\")\n",
        "\n",
        "    for i, (doc_id, doc) in enumerate(list(documents.items())[:num_samples]):\n",
        "        print(f\"\\nDocument ID: {doc['id']}\")\n",
        "        print(f\"Title: {doc['title'][:100]}...\")\n",
        "        print(f\"Author: {doc['author']}\")\n",
        "        print(f\"Content Preview: {doc['content'][:150]}...\")\n",
        "\n",
        "\n",
        "\n",
        "    print(\"SAMPLE QUERIES\")\n",
        "\n",
        "    for i, (query_id, query) in enumerate(list(queries.items())[:num_samples]):\n",
        "        print(f\"\\nQuery ID: {query['id']}\")\n",
        "        print(f\"Text: {query['text']}\")\n",
        "\n",
        "\n",
        "\n",
        "    print(\"SAMPLE RELEVANCE JUDGMENTS\")\n",
        "\n",
        "    for i, (query_id, doc_ids) in enumerate(list(relevance.items())[:num_samples]):\n",
        "        print(f\"\\nQuery {query_id} has {len(doc_ids)} relevant documents:\")\n",
        "        print(f\"Relevant Doc IDs: {doc_ids[:10]}...\")  # Show first 10\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    documents, queries, relevance = load_cisi_dataset('cisi_data')\n",
        "\n",
        "\n",
        "    if documents and queries and relevance:\n",
        "        display_samples(documents, queries, relevance, num_samples=3)\n",
        "\n",
        "        print(\"\\nPhase 1 Complete: Data successfully parsed!\")\n",
        "        print(\" You can now access:\")\n",
        "        print(\" documents[1] → Get document 1\")\n",
        "        print(\" queries[5] → Get query 5\")\n",
        "        print(\" relevance[1] → Get relevant docs for query 1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgogUAyKSUaG",
        "outputId": "c28498b2-cb16-4676-ab49-6059c3bfcd09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADING CISI DATASET\n",
            "Successfully parsed 1460 documents from CISI.ALL\n",
            " Successfully parsed 112 queries from CISI.QRY\n",
            "Successfully parsed relevance judgments for 76 queries from CISI.REL\n",
            "DATASET SUMMARY\n",
            "Total Documents: 1460\n",
            "Total Queries: 112\n",
            "Queries with Relevance Judgments: 76\n",
            "SAMPLE DOCUMENTS\n",
            "\n",
            "Document ID: 1\n",
            "Title: 18 Editions of the Dewey Decimal Classifications...\n",
            "Author: Comaromi, J.P.\n",
            "Content Preview: 18 Editions of the Dewey Decimal Classifications The present study is a history of the DEWEY Decimal\n",
            "Classification.  The first edition of the DDC was...\n",
            "\n",
            "Document ID: 2\n",
            "Title: Use Made of Technical Libraries...\n",
            "Author: Slater, M.\n",
            "Content Preview: Use Made of Technical Libraries This report is an analysis of 6300 acts of use\n",
            "in 104 technical libraries in the United Kingdom.\n",
            "Library use is only o...\n",
            "\n",
            "Document ID: 3\n",
            "Title: Two Kinds of Power\n",
            "An Essay on Bibliographic Control...\n",
            "Author: Wilson, P.\n",
            "Content Preview: Two Kinds of Power\n",
            "An Essay on Bibliographic Control The relationships between the organization and control of writings\n",
            "and the organization and contr...\n",
            "SAMPLE QUERIES\n",
            "\n",
            "Query ID: 1\n",
            "Text: What problems and concerns are there in making up descriptive titles?\n",
            "What difficulties are involved in automatically retrieving articles from\n",
            "approximate titles?\n",
            "What is the usual relevance of the content of articles to their titles?\n",
            "\n",
            "Query ID: 2\n",
            "Text: How can actually pertinent data, as opposed to references or entire articles\n",
            "themselves, be retrieved automatically in response to information requests?\n",
            "\n",
            "Query ID: 3\n",
            "Text: What is information science?  Give definitions where possible.\n",
            "SAMPLE RELEVANCE JUDGMENTS\n",
            "\n",
            "Query 1 has 46 relevant documents:\n",
            "Relevant Doc IDs: [28, 35, 38, 42, 43, 52, 65, 76, 86, 150]...\n",
            "------------------------------------------------------------\n",
            "\n",
            "Query 2 has 26 relevant documents:\n",
            "Relevant Doc IDs: [29, 68, 197, 213, 214, 309, 319, 324, 429, 499]...\n",
            "------------------------------------------------------------\n",
            "\n",
            "Query 3 has 44 relevant documents:\n",
            "Relevant Doc IDs: [60, 85, 114, 123, 126, 131, 133, 136, 138, 140]...\n",
            "------------------------------------------------------------\n",
            "\n",
            "Phase 1 Complete: Data successfully parsed!\n",
            " You can now access:\n",
            " documents[1] → Get document 1\n",
            " queries[5] → Get query 5\n",
            " relevance[1] → Get relevant docs for query 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PHASE 2: TEXT PREPROCESSING FOR INFORMATION RETRIEVAL**"
      ],
      "metadata": {
        "id": "aPR49jPsTGXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install nltk\n",
        "\n",
        "\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "print(\"Libraries installed and imported successfully!\")"
      ],
      "metadata": {
        "id": "8gtb0pobTXwM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3b38b6b-57a4-47bc-f85a-0e22795becfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Libraries installed and imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.stemmer = PorterStemmer()\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        \"\"\"Clean and process text into tokens.\"\"\"\n",
        "        if not text:\n",
        "            return []\n",
        "\n",
        "\n",
        "        text = text.lower()\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "\n",
        "        tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "\n",
        "\n",
        "        tokens = [t for t in tokens if t not in self.stop_words and len(t) >= 2]\n",
        "\n",
        "\n",
        "        tokens = [self.stemmer.stem(t) for t in tokens]\n",
        "\n",
        "        return tokens"
      ],
      "metadata": {
        "id": "zBRijoo6UQy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = TextPreprocessor()\n",
        "test = \"The RUNNING dogs are running QUICKLY!\"\n",
        "result = preprocessor.preprocess(test)\n",
        "\n",
        "print(\"Preprocessor created!\")\n",
        "print(f\"\\nTest: '{test}'\")\n",
        "print(f\"Result: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbUUXdUhikiW",
        "outputId": "fed61e41-ccda-4555-df49-0d8c715bc432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessor created!\n",
            "\n",
            "Test: 'The RUNNING dogs are running QUICKLY!'\n",
            "Result: ['run', 'dog', 'run', 'quickli']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREPROCESSING ALL DOCUMENTS AND QUERIES**"
      ],
      "metadata": {
        "id": "QTWsNhl6WQYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_docs = {}\n",
        "\n",
        "\n",
        "print(\"PREPROCESSING DOCUMENTS\")\n",
        "\n",
        "\n",
        "for doc_id, doc in documents.items():\n",
        "    tokens = preprocessor.preprocess(doc['content'])\n",
        "    preprocessed_docs[doc_id] = {\n",
        "        'id': doc_id,\n",
        "        'tokens': tokens,\n",
        "        'original_title': doc['title']\n",
        "    }\n",
        "\n",
        "\n",
        "    if doc_id % 200 == 0:\n",
        "        print(f\"  Processed {doc_id}/{len(documents)} documents...\")\n",
        "\n",
        "print(f\"\\nPreprocessed {len(preprocessed_docs)} documents!\")\n",
        "\n",
        "\n",
        "print(\"\\n EXAMPLE - Document 1:\")\n",
        "print(f\"Original: {documents[1]['content'][:150]}...\")\n",
        "print(f\"Tokens: {preprocessed_docs[1]['tokens'][:15]}\")\n",
        "print(f\"Total tokens: {len(preprocessed_docs[1]['tokens'])}\")"
      ],
      "metadata": {
        "id": "v9EwzQqiWUFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1063cede-64fa-48de-fbd2-1eae6d4f03de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREPROCESSING DOCUMENTS\n",
            "  Processed 200/1460 documents...\n",
            "  Processed 400/1460 documents...\n",
            "  Processed 600/1460 documents...\n",
            "  Processed 800/1460 documents...\n",
            "  Processed 1000/1460 documents...\n",
            "  Processed 1200/1460 documents...\n",
            "  Processed 1400/1460 documents...\n",
            "\n",
            "Preprocessed 1460 documents!\n",
            "\n",
            " EXAMPLE - Document 1:\n",
            "Original: 18 Editions of the Dewey Decimal Classifications The present study is a history of the DEWEY Decimal\n",
            "Classification.  The first edition of the DDC was...\n",
            "Tokens: ['18', 'edit', 'dewey', 'decim', 'classif', 'present', 'studi', 'histori', 'dewey', 'decim', 'classif', 'first', 'edit', 'ddc', 'publish']\n",
            "Total tokens: 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_queries = {}\n",
        "\n",
        "print(\"PREPROCESSING QUERIES\")\n",
        "\n",
        "\n",
        "for query_id, query in queries.items():\n",
        "    tokens = preprocessor.preprocess(query['text'])\n",
        "    preprocessed_queries[query_id] = {\n",
        "        'id': query_id,\n",
        "        'tokens': tokens,\n",
        "        'original_text': query['text']\n",
        "    }\n",
        "\n",
        "print(f\"Preprocessed {len(preprocessed_queries)} queries!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9i39pBsdpHk",
        "outputId": "38f022e6-7ba3-44da-9f97-94c6883d02f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREPROCESSING QUERIES\n",
            "Preprocessed 112 queries!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statistics**"
      ],
      "metadata": {
        "id": "o4j58pyPjJIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = []\n",
        "for doc in preprocessed_docs.values():\n",
        "    all_tokens.extend(doc['tokens'])\n",
        "\n",
        "vocabulary = set(all_tokens)\n",
        "token_freq = Counter(all_tokens)\n",
        "\n",
        "\n",
        "print(\"PREPROCESSING STATISTICS\")\n",
        "\n",
        "\n",
        "print(f\"\\n DOCUMENTS:\")\n",
        "print(f\"  Total documents: {len(preprocessed_docs)}\")\n",
        "print(f\"  Total tokens: {len(all_tokens):,}\")\n",
        "print(f\"  Unique words (vocabulary): {len(vocabulary):,}\")\n",
        "print(f\"  Average tokens per doc: {len(all_tokens)/len(preprocessed_docs):.1f}\")\n",
        "\n",
        "print(f\"\\n QUERIES:\")\n",
        "total_query_tokens = sum(len(q['tokens']) for q in preprocessed_queries.values())\n",
        "print(f\"  Total queries: {len(preprocessed_queries)}\")\n",
        "print(f\"  Average tokens per query: {total_query_tokens/len(preprocessed_queries):.1f}\")\n",
        "\n",
        "print(f\"\\n TOP 10 MOST COMMON WORDS:\")\n",
        "for word, count in token_freq.most_common(10):\n",
        "    print(f\"  {word}: {count:,} times\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NihRobA8jG6y",
        "outputId": "4d823625-3173-4dc7-f137-f203a5627af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREPROCESSING STATISTICS\n",
            "\n",
            " DOCUMENTS:\n",
            "  Total documents: 1460\n",
            "  Total tokens: 262,301\n",
            "  Unique words (vocabulary): 8,160\n",
            "  Average tokens per doc: 179.7\n",
            "\n",
            " QUERIES:\n",
            "  Total queries: 112\n",
            "  Average tokens per query: 47.1\n",
            "\n",
            " TOP 10 MOST COMMON WORDS:\n",
            "  librari: 1,866 times\n",
            "  inform: 1,644 times\n",
            "  system: 1,250 times\n",
            "  use: 1,132 times\n",
            "  index: 695 times\n",
            "  research: 617 times\n",
            "  retriev: 603 times\n",
            "  data: 585 times\n",
            "  studi: 572 times\n",
            "  175: 551 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n VERIFICATION:\")\n",
        "print(f\" preprocessed_docs created: {len(preprocessed_docs)} documents\")\n",
        "print(f\"preprocessed_queries created: {len(preprocessed_queries)} queries\")\n",
        "print(f\"preprocessor created: {type(preprocessor)}\")\n",
        "\n",
        "\n",
        "print(\"\\n Quick Access Test:\")\n",
        "print(f\"Document 5 has {len(preprocessed_docs[5]['tokens'])} tokens\")\n",
        "print(f\"Query 10 has {len(preprocessed_queries[10]['tokens'])} tokens\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKa0Pzy6gEBJ",
        "outputId": "8b7682e5-cbc1-42a8-fe42-48bcf1637311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " VERIFICATION:\n",
            " preprocessed_docs created: 1460 documents\n",
            "preprocessed_queries created: 112 queries\n",
            "preprocessor created: <class '__main__.TextPreprocessor'>\n",
            "\n",
            " Quick Access Test:\n",
            "Document 5 has 136 tokens\n",
            "Query 10 has 8 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phase 3: Building the Inverted Index**"
      ],
      "metadata": {
        "id": "mqwZZX7pj4r5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "print(\"BUILDING INVERTED INDEX\")\n",
        "\n",
        "\n",
        "inverted_index = defaultdict(list)\n",
        "\n",
        "for doc_id, doc in preprocessed_docs.items():\n",
        "    for token in doc['tokens']:\n",
        "        if doc_id not in inverted_index[token]:\n",
        "            inverted_index[token].append(doc_id)\n",
        "\n",
        "\n",
        "inverted_index = dict(inverted_index)\n",
        "\n",
        "print(f\" Inverted index created!\")\n",
        "print(f\"   Total unique words: {len(inverted_index):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chsCL9Qtj4T2",
        "outputId": "0858fb13-9f0c-4791-c0b9-d84f3f97d074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BUILDING INVERTED INDEX\n",
            " Inverted index created!\n",
            "   Total unique words: 8,160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate Term Frequency (TF)**"
      ],
      "metadata": {
        "id": "GpW7Wg0JmChk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"CALCULATING TERM FREQUENCIES\")\n",
        "\n",
        "\n",
        "\n",
        "document_tf = {}\n",
        "\n",
        "for doc_id, doc in preprocessed_docs.items():\n",
        "    tf = Counter(doc['tokens'])\n",
        "    document_tf[doc_id] = dict(tf)\n",
        "\n",
        "print(f\"Term frequencies calculated for {len(document_tf)} documents!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvkMQiqMj1am",
        "outputId": "a6cee486-474e-4526-e051-2a69401d945f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CALCULATING TERM FREQUENCIES\n",
            "Term frequencies calculated for 1460 documents!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Inverse Document Frequency (IDF)"
      ],
      "metadata": {
        "id": "xeSdMqeJmPyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "\n",
        "print(\"CALCULATING IDF SCORES\")\n",
        "\n",
        "\n",
        "total_docs = len(preprocessed_docs)\n",
        "idf = {}\n",
        "\n",
        "for term, doc_list in inverted_index.items():\n",
        "    df = len(doc_list)\n",
        "    idf[term] = math.log(total_docs / df)\n",
        "\n",
        "print(f\" IDF calculated for {len(idf):,} terms!\")\n",
        "\n",
        "\n",
        "print(\"\\n IDF EXAMPLES:\")\n",
        "print(\"   (Higher IDF = More rare/important word)\")\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS61qlfshtgS",
        "outputId": "d6e49d5e-4ecd-48d2-c708-ad874361740b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CALCULATING IDF SCORES\n",
            " IDF calculated for 8,160 terms!\n",
            "\n",
            " IDF EXAMPLES:\n",
            "   (Higher IDF = More rare/important word)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate TF-IDF Scores**"
      ],
      "metadata": {
        "id": "OBgunh_OmdCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CALCULATING TF-IDF SCORES\")\n",
        "\n",
        "document_tfidf = {}\n",
        "\n",
        "for doc_id in preprocessed_docs.keys():\n",
        "    tfidf = {}\n",
        "    tf_dict = document_tf[doc_id]\n",
        "\n",
        "    for term, tf_value in tf_dict.items():\n",
        "        if term in idf:\n",
        "            tfidf[term] = tf_value * idf[term]\n",
        "\n",
        "    document_tfidf[doc_id] = tfidf\n",
        "\n",
        "print(f\" TF-IDF calculated for {len(document_tfidf)} documents!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qgvLl5Zh-aL",
        "outputId": "f6300c3c-23b5-489d-9189-5cc93ee65707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CALCULATING TF-IDF SCORES\n",
            " TF-IDF calculated for 1460 documents!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Search Function (TF-IDF)**"
      ],
      "metadata": {
        "id": "SvbRRczDmo7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_tfidf(query_text, top_k=10):\n",
        "\n",
        "    query_tokens = preprocessor.preprocess(query_text)\n",
        "\n",
        "\n",
        "    scores = {}\n",
        "    for doc_id in preprocessed_docs.keys():\n",
        "        score = 0\n",
        "        for term in query_tokens:\n",
        "            if term in document_tfidf[doc_id]:\n",
        "                score += document_tfidf[doc_id][term]\n",
        "\n",
        "        if score > 0:\n",
        "            scores[doc_id] = score\n",
        "\n",
        "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "    return ranked\n",
        "\n",
        "print(\" Search function created!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M72D1YfmuOs",
        "outputId": "19982d33-6d2c-4f02-dc2e-bdc34e90a9d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Search function created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_query = \"information retrieval system\"\n",
        "print(f\"\\nSearching for: '{test_query}'\")\n",
        "\n",
        "\n",
        "results = search_tfidf(test_query, top_k=5)\n",
        "\n",
        "print(f\"\\n TOP 5 RESULTS:\\n\")\n",
        "for rank, (doc_id, score) in enumerate(results, 1):\n",
        "    title = preprocessed_docs[doc_id]['original_title']\n",
        "    print(f\"{rank}. Document {doc_id} (Score: {score:.2f})\")\n",
        "    print(f\"   Title: {title[:80]}...\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQk1LlOlm4CI",
        "outputId": "c9dd949e-9c3e-4510-b629-93c02eb164f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Searching for: 'information retrieval system'\n",
            "\n",
            " TOP 5 RESULTS:\n",
            "\n",
            "1. Document 636 (Score: 26.14)\n",
            "   Title: Text Searching Retrieval of Answer-Sentences and Other Answer-Passages...\n",
            "\n",
            "2. Document 523 (Score: 20.73)\n",
            "   Title: The Cost_Performance of an On-Line, Free-Text Bibliographic Retrieval System...\n",
            "\n",
            "3. Document 630 (Score: 20.70)\n",
            "   Title: A Novel Philosophy for the Design of Information Storage\n",
            "and Retrieval Systems A...\n",
            "\n",
            "4. Document 1136 (Score: 20.49)\n",
            "   Title: Data Retrieval Systems:  Specifics and Problems...\n",
            "\n",
            "5. Document 615 (Score: 18.54)\n",
            "   Title: A Cost Model for Evaluating Information Retrieval Systems...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RETRIEVAL MODELS & QUERY PROCESSING**"
      ],
      "metadata": {
        "id": "NTFGtE-r9GLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "print(\"CALCULATING IDF VALUES\")\n",
        "\n",
        "\n",
        "idf = {}\n",
        "N = len(preprocessed_docs)\n",
        "\n",
        "for term, postings in inverted_index.items():\n",
        "    df = len(postings)\n",
        "    idf[term] = math.log(N / df)\n",
        "\n",
        "print(f\" IDF calculated for {len(idf)} unique terms\")\n",
        "print(f\"\\nSample IDF values:\")\n",
        "sample_terms = list(idf.items())[:5]\n",
        "for term, idf_val in sample_terms:\n",
        "    print(f\"  {term}: {idf_val:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUA8TT6T9KXg",
        "outputId": "dc3c6c29-9161-4c3d-f529-be1909714acb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CALCULATING IDF VALUES\n",
            " IDF calculated for 8160 unique terms\n",
            "\n",
            "Sample IDF values:\n",
            "  18: 2.3233\n",
            "  edit: 3.5485\n",
            "  dewey: 4.8013\n",
            "  decim: 4.5136\n",
            "  classif: 2.6322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate Document TF-IDF Vectors**"
      ],
      "metadata": {
        "id": "nHhN1o4yAM5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(\"\\nCALCULATING DOCUMENT TF-IDF VECTORS\")\n",
        "\n",
        "doc_vectors = {}\n",
        "\n",
        "for doc_id, doc in preprocessed_docs.items():\n",
        "    tokens = doc['tokens']\n",
        "    doc_length = len(tokens)\n",
        "    term_freq = Counter(tokens)\n",
        "\n",
        "\n",
        "    vector = {}\n",
        "    for term, freq in term_freq.items():\n",
        "        tf = freq / doc_length\n",
        "        vector[term] = tf * idf.get(term, 0)\n",
        "\n",
        "    doc_vectors[doc_id] = vector\n",
        "\n",
        "print(f\"TF-IDF vectors created for {len(doc_vectors)} documents\")\n",
        "print(f\"\\nDocument 1 vector (first 5 terms):\")\n",
        "for i, (term, score) in enumerate(list(doc_vectors[1].items())[:5]):\n",
        "    print(f\"  {term}: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d7I5G4fAJnm",
        "outputId": "64be2206-caf0-4692-dbf7-c0860efe159a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CALCULATING DOCUMENT TF-IDF VECTORS\n",
            "TF-IDF vectors created for 1460 documents\n",
            "\n",
            "Document 1 vector (first 5 terms):\n",
            "  18: 0.0415\n",
            "  edit: 0.2535\n",
            "  dewey: 0.2572\n",
            "  decim: 0.1612\n",
            "  classif: 0.0940\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nBUILDING SEARCH FUNCTION\")\n",
        "\n",
        "def simple_search(query_text, top_k=10):\n",
        "    \"\"\"Simple TF-IDF search\"\"\"\n",
        "\n",
        "\n",
        "    query_tokens = preprocessor.preprocess(query_text)\n",
        "    print(f\"Query tokens: {query_tokens}\")\n",
        "\n",
        "\n",
        "    query_freq = Counter(query_tokens)\n",
        "    query_length = len(query_tokens)\n",
        "\n",
        "    query_vector = {}\n",
        "    for term, freq in query_freq.items():\n",
        "        if term in idf:\n",
        "            tf = freq / query_length\n",
        "            query_vector[term] = tf * idf[term]\n",
        "\n",
        "    scores = {}\n",
        "    for doc_id, doc_vector in doc_vectors.items():\n",
        "\n",
        "        score = 0\n",
        "        for term in query_vector:\n",
        "            if term in doc_vector:\n",
        "                score += query_vector[term] * doc_vector[term]\n",
        "\n",
        "        if score > 0:\n",
        "            scores[doc_id] = score\n",
        "\n",
        "\n",
        "    results = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return results[:top_k]\n",
        "\n",
        "print(\" Search function ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4M4j0qCAQWw",
        "outputId": "fc2ad39f-6724-48f9-89a9-4283979b7142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BUILDING SEARCH FUNCTION\n",
            " Search function ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TEST THE SEARCH!**"
      ],
      "metadata": {
        "id": "-in5GspVAVyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING SEARCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "query = \"information retrieval\"\n",
        "results = simple_search(query, top_k=10)\n",
        "\n",
        "print(f\"\\nQuery: '{query}'\")\n",
        "print(f\"Found {len(results)} documents\\n\")\n",
        "\n",
        "\n",
        "for rank, (doc_id, score) in enumerate(results[:5], 1):\n",
        "    title = preprocessed_docs[doc_id]['original_title']\n",
        "    print(f\"{rank}. Doc {doc_id} (Score: {score:.4f})\")\n",
        "    print(f\"   {title[:70]}...\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGOAbG_yASqq",
        "outputId": "d1729199-9df4-42aa-c665-1471938e5236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TESTING SEARCH\n",
            "============================================================\n",
            "Query tokens: ['inform', 'retriev']\n",
            "\n",
            "Query: 'information retrieval'\n",
            "Found 10 documents\n",
            "\n",
            "1. Doc 539 (Score: 0.1512)\n",
            "   Information Retrieval Languages...\n",
            "\n",
            "2. Doc 1136 (Score: 0.1178)\n",
            "   Data Retrieval Systems:  Specifics and Problems...\n",
            "\n",
            "3. Doc 1134 (Score: 0.0966)\n",
            "   Information Retrieval Learning...\n",
            "\n",
            "4. Doc 1120 (Score: 0.0712)\n",
            "   A Grammatical Elements in a Descriptor Language for an \n",
            "Information Re...\n",
            "\n",
            "5. Doc 1171 (Score: 0.0659)\n",
            "   Problems of Compatibility of Information on Retrieval Systems and \n",
            "Req...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query_id = 1\n",
        "query_text = queries[query_id]['text']\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"TESTING WITH CISI QUERY {query_id}\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Query: {query_text}\\n\")\n",
        "\n",
        "results = simple_search(query_text, top_k=10)\n",
        "\n",
        "\n",
        "print(\"Top 10 Retrieved Documents:\")\n",
        "retrieved_ids = []\n",
        "for rank, (doc_id, score) in enumerate(results, 1):\n",
        "    retrieved_ids.append(doc_id)\n",
        "    print(f\"{rank}. Doc {doc_id} (Score: {score:.4f})\")\n",
        "\n",
        "\n",
        "print(f\"\\n📊 EVALUATION:\")\n",
        "print(f\"Retrieved: {retrieved_ids}\")\n",
        "print(f\"Relevant (ground truth): {relevance[query_id][:10]}\")\n",
        "\n",
        "# Calculate overlap\n",
        "overlap = set(retrieved_ids) & set(relevance[query_id])\n",
        "print(f\" Found {len(overlap)} relevant documents out of 10!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcVFp9wpAdz5",
        "outputId": "ea907ab7-1d76-4c87-f8b3-f62317f67da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TESTING WITH CISI QUERY 1\n",
            "============================================================\n",
            "Query: What problems and concerns are there in making up descriptive titles?\n",
            "What difficulties are involved in automatically retrieving articles from\n",
            "approximate titles?\n",
            "What is the usual relevance of the content of articles to their titles?\n",
            "\n",
            "Query tokens: ['problem', 'concern', 'make', 'descript', 'titl', 'difficulti', 'involv', 'automat', 'retriev', 'articl', 'approxim', 'titl', 'usual', 'relev', 'content', 'articl', 'titl']\n",
            "Top 10 Retrieved Documents:\n",
            "1. Doc 429 (Score: 0.0571)\n",
            "2. Doc 589 (Score: 0.0553)\n",
            "3. Doc 276 (Score: 0.0478)\n",
            "4. Doc 1064 (Score: 0.0430)\n",
            "5. Doc 322 (Score: 0.0419)\n",
            "6. Doc 722 (Score: 0.0397)\n",
            "7. Doc 956 (Score: 0.0383)\n",
            "8. Doc 869 (Score: 0.0376)\n",
            "9. Doc 805 (Score: 0.0369)\n",
            "10. Doc 1323 (Score: 0.0352)\n",
            "\n",
            "📊 EVALUATION:\n",
            "Retrieved: [429, 589, 276, 1064, 322, 722, 956, 869, 805, 1323]\n",
            "Relevant (ground truth): [28, 35, 38, 42, 43, 52, 65, 76, 86, 150]\n",
            " Found 4 relevant documents out of 10!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Week 7 Assignment**"
      ],
      "metadata": {
        "id": "WhmI8IM4DavI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STEP 1: CALCULATING PRECISION\")\n",
        "\n",
        "def precision_at_k(retrieved, relevant, k=10):\n",
        "\n",
        "    retrieved_k = retrieved[:k]\n",
        "    relevant_set = set(relevant)\n",
        "\n",
        "\n",
        "    relevant_retrieved = sum(1 for doc_id in retrieved_k if doc_id in relevant_set)\n",
        "\n",
        "    precision = relevant_retrieved / k if k > 0 else 0\n",
        "    return precision\n",
        "\n",
        "\n",
        "query_id = 1\n",
        "results = simple_search(queries[query_id]['text'], top_k=10)\n",
        "retrieved = [doc_id for doc_id, score in results]\n",
        "relevant = relevance[query_id]\n",
        "\n",
        "precision = precision_at_k(retrieved, relevant, k=10)\n",
        "\n",
        "print(f\"\\nQuery {query_id}: {queries[query_id]['text'][:50]}...\")\n",
        "print(f\"Retrieved: {retrieved}\")\n",
        "print(f\"Relevant:  {relevant[:10]}...\")\n",
        "print(f\"Precision@10: {precision:.4f} ({precision*100:.1f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0Rx6PX5DZqH",
        "outputId": "93b9602e-e776-43f9-b252-b4dccb19af52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 1: CALCULATING PRECISION\n",
            "Query tokens: ['problem', 'concern', 'make', 'descript', 'titl', 'difficulti', 'involv', 'automat', 'retriev', 'articl', 'approxim', 'titl', 'usual', 'relev', 'content', 'articl', 'titl']\n",
            "\n",
            "Query 1: What problems and concerns are there in making up ...\n",
            "Retrieved: [429, 589, 276, 1064, 322, 722, 956, 869, 805, 1323]\n",
            "Relevant:  [28, 35, 38, 42, 43, 52, 65, 76, 86, 150]...\n",
            "Precision@10: 0.4000 (40.0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recall@K Function**"
      ],
      "metadata": {
        "id": "w69c9p2gDkzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSTEP 2: CALCULATING RECALL\")\n",
        "\n",
        "def recall_at_k(retrieved, relevant, k=10):\n",
        "\n",
        "    retrieved_k = retrieved[:k]\n",
        "    relevant_set = set(relevant)\n",
        "\n",
        "\n",
        "    relevant_retrieved = sum(1 for doc_id in retrieved_k if doc_id in relevant_set)\n",
        "\n",
        "    recall = relevant_retrieved / len(relevant) if len(relevant) > 0 else 0\n",
        "    return recall\n",
        "\n",
        "\n",
        "recall = recall_at_k(retrieved, relevant, k=10)\n",
        "\n",
        "print(f\"\\nQuery {query_id}: {queries[query_id]['text'][:50]}...\")\n",
        "print(f\"Retrieved {len(retrieved[:10])} documents\")\n",
        "print(f\"Total relevant documents: {len(relevant)}\")\n",
        "print(f\"Found: {sum(1 for doc in retrieved[:10] if doc in relevant)}\")\n",
        "print(f\" Recall@10: {recall:.4f} ({recall*100:.1f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdoit8KoAeOn",
        "outputId": "6e766bc9-1d44-4f20-a074-b67ebe8875f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 2: CALCULATING RECALL\n",
            "\n",
            "Query 1: What problems and concerns are there in making up ...\n",
            "Retrieved 10 documents\n",
            "Total relevant documents: 46\n",
            "Found: 4\n",
            " Recall@10: 0.0870 (8.7%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average Precision (for MAP)\n",
        "**bold text**"
      ],
      "metadata": {
        "id": "oqSB7qA_Du81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSTEP 3: CALCULATING AVERAGE PRECISION\")\n",
        "\n",
        "def average_precision(retrieved, relevant):\n",
        "\n",
        "    relevant_set = set(relevant)\n",
        "\n",
        "    precision_sum = 0\n",
        "    relevant_count = 0\n",
        "\n",
        "    for i, doc_id in enumerate(retrieved, 1):\n",
        "        if doc_id in relevant_set:\n",
        "            relevant_count += 1\n",
        "            precision_at_i = relevant_count / i\n",
        "            precision_sum += precision_at_i\n",
        "\n",
        "    if relevant_count == 0:\n",
        "        return 0\n",
        "\n",
        "    ap = precision_sum / len(relevant)\n",
        "    return ap\n",
        "\n",
        "\n",
        "ap = average_precision(retrieved, relevant)\n",
        "\n",
        "print(f\"\\nQuery {query_id}\")\n",
        "print(f\" Average Precision: {ap:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6NTGxNuDxRo",
        "outputId": "1386414e-3a5d-44e9-f180-59db67284efd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 3: CALCULATING AVERAGE PRECISION\n",
            "\n",
            "Query 1\n",
            " Average Precision: 0.0652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAP (Mean Average Precision)\n",
        "**bold text**"
      ],
      "metadata": {
        "id": "N2YYYfS2D6kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSTEP 4: CALCULATING MAP (MEAN AVERAGE PRECISION)\")\n",
        "\n",
        "def calculate_map(queries_dict, relevance_dict, top_k=10):\n",
        "\n",
        "    ap_scores = []\n",
        "\n",
        "    for query_id in relevance_dict.keys():\n",
        "        if query_id not in queries_dict:\n",
        "            continue\n",
        "\n",
        "\n",
        "        query_text = queries_dict[query_id]['text']\n",
        "        results = simple_search(query_text, top_k=top_k)\n",
        "        retrieved = [doc_id for doc_id, score in results]\n",
        "\n",
        "\n",
        "        relevant = relevance_dict[query_id]\n",
        "        ap = average_precision(retrieved, relevant)\n",
        "        ap_scores.append(ap)\n",
        "\n",
        "\n",
        "        if query_id % 20 == 0:\n",
        "            print(f\"  Evaluated {query_id} queries...\")\n",
        "\n",
        "    map_score = sum(ap_scores) / len(ap_scores) if ap_scores else 0\n",
        "    return map_score, ap_scores\n",
        "\n",
        "\n",
        "print(\"Evaluating all queries...\")\n",
        "map_score, ap_scores = calculate_map(queries, relevance, top_k=10)\n",
        "\n",
        "print(f\"\\n MAP@10: {map_score:.4f}\")\n",
        "print(f\"   Evaluated {len(ap_scores)} queries\")\n",
        "print(f\"   Best AP: {max(ap_scores):.4f}\")\n",
        "print(f\"   Worst AP: {min(ap_scores):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8fxgMraD9mF",
        "outputId": "2ff23098-7d74-4670-963c-d81281daee41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 4: CALCULATING MAP (MEAN AVERAGE PRECISION)\n",
            "Evaluating all queries...\n",
            "Query tokens: ['problem', 'concern', 'make', 'descript', 'titl', 'difficulti', 'involv', 'automat', 'retriev', 'articl', 'approxim', 'titl', 'usual', 'relev', 'content', 'articl', 'titl']\n",
            "Query tokens: ['actual', 'pertin', 'data', 'oppos', 'refer', 'entir', 'articl', 'retriev', 'automat', 'respons', 'inform', 'request']\n",
            "Query tokens: ['inform', 'scienc', 'give', 'definit', 'possibl']\n",
            "Query tokens: ['imag', 'recognit', 'method', 'automat', 'transform', 'print', 'text', 'computerreadi', 'form']\n",
            "Query tokens: ['special', 'train', 'ordinari', 'research', 'businessmen', 'need', 'proper', 'inform', 'manag', 'unobstruct', 'use', 'inform', 'retriev', 'system', 'problem', 'like', 'encount']\n",
            "Query tokens: ['possibl', 'verbal', 'commun', 'comput', 'human', 'commun', 'via', 'spoken', 'word']\n",
            "Query tokens: ['describ', 'present', 'work', 'plan', 'system', 'publish', 'print', 'origin', 'paper', 'comput', 'save', 'byproduct', 'articl', 'code', 'dataprocess', 'form', 'use', 'retriev']\n",
            "Query tokens: ['describ', 'inform', 'retriev', 'index', 'languag', 'bear', 'scienc', 'gener']\n",
            "Query tokens: ['possibl', 'automat', 'grammat', 'contextu', 'analysi', 'articl', 'inclus', 'inform', 'retriev', 'system']\n",
            "Query tokens: ['use', 'abstract', 'mathemat', 'inform', 'retriev', 'eg', 'group', 'theori']\n",
            "Query tokens: ['need', 'inform', 'consolid', 'evalu', 'retriev', 'scientif', 'research']\n",
            "Query tokens: ['give', 'method', 'high', 'speed', 'public', 'print', 'distribut', 'scientif', 'journal']\n",
            "Query tokens: ['criteria', 'develop', 'object', 'evalu', 'inform', 'retriev', 'dissemin', 'system']\n",
            "Query tokens: ['futur', 'automat', 'medic', 'diagnosi']\n",
            "Query tokens: ['much', 'inform', 'retriev', 'dissemin', 'system', 'well', 'autom', 'librari', 'cost', 'worth', 'research', 'industri']\n",
            "Query tokens: ['system', 'incorpor', 'multiprogram', 'remot', 'station', 'inform', 'retriev', 'extent', 'use', 'futur']\n",
            "Query tokens: ['mean', 'obtain', 'larg', 'volum', 'high', 'speed', 'custom', 'usabl', 'inform', 'retriev', 'output']\n",
            "Query tokens: ['method', 'encod', 'automat', 'match', 'automat', 'draw', 'structur', 'extend', 'two', 'dimens', 'like', 'structur', 'formula', 'chemic', 'compound']\n",
            "Query tokens: ['techniqu', 'machin', 'match', 'machin', 'search', 'system', 'code', 'match', 'method']\n",
            "Query tokens: ['test', 'autom', 'inform', 'system']\n",
            "  Evaluated 20 queries...\n",
            "Query tokens: ['need', 'provid', 'personnel', 'inform', 'field']\n",
            "Query tokens: ['autom', 'inform', 'medic', 'field']\n",
            "Query tokens: ['amount', 'use', 'book', 'librari', 'relat', 'need', 'autom', 'inform', 'system']\n",
            "Query tokens: ['educ', 'train', 'requir', 'personnel', 'inform', 'field', 'possibl', 'train', 'need', 'program', 'provid', 'train']\n",
            "Query tokens: ['intern', 'system', 'exchang', 'dissemin', 'inform']\n",
            "Query tokens: ['cost', 'determin', 'cost', 'associ', 'system', 'autom', 'inform']\n",
            "Query tokens: ['computer', 'inform', 'retriev', 'system', 'computer', 'index', 'system']\n",
            "Query tokens: ['computer', 'inform', 'system', 'field', 'relat', 'chemistri']\n",
            "Query tokens: ['specif', 'advantag', 'computer', 'index', 'system']\n",
            "Query tokens: ['inform', 'dissemin', 'journal', 'period']\n",
            "Query tokens: ['inform', 'system', 'physic', 'scienc']\n",
            "Query tokens: ['attempt', 'computer', 'mechan', 'system', 'gener', 'librari', 'problem', 'method', 'autom', 'gener', 'author', 'titl', 'index', 'system']\n",
            "Query tokens: ['retriev', 'system', 'provid', 'autom', 'transmiss', 'inform', 'user', 'distanc']\n",
            "Query tokens: ['method', 'code', 'use', 'computer', 'index', 'system']\n",
            "Query tokens: ['govern', 'support', 'agenc', 'project', 'deal', 'inform', 'dissemin']\n",
            "Query tokens: ['list', 'word', 'use', 'index', 'classifi', 'materi', 'avail', 'want', 'list', 'term', 'descript', 'vocabulari', 'particular', 'field', 'schedul', 'word', 'relat', 'meaning', 'scheme', 'want', 'list', 'test', 'least', 'extent', 'found', 'use', 'organ', 'materi', 'retriev']\n",
            "Query tokens: ['progress', 'inform', 'retriev', 'present', 'problem', 'maladjust', 'disloc', 'personnel', 'train', 'retrain', 'peopl', 'use', 'new', 'equip', 'import', 'level', 'librarian', 'assist', 'technician', 'student', 'research', 'even', 'execut', 'need', 'educ', 'learn', 'purpos', 'valu', 'use', 'inform', 'system', 'hardwar', 'program', 'develop', 'chang', 'attitud', 'skill', 'tradit', 'worker', 'help', 'learn', 'newer', 'techniqu']\n",
            "Query tokens: ['alphabet', 'order', 'materi', 'consid', 'use', 'tool', 'inform', 'retriev', 'studi', 'done', 'compar', 'effect', 'alphabet', 'order', 'organ', 'scheme', 'gener', 'accept', 'form', 'arrang', 'materi', 'alphabet', 'order', 'easi', 'way', 'achiev', 'form', 'without', 'go', 'great', 'amount', 'effort']\n",
            "Query tokens: ['averag', 'student', 'research', 'difficulti', 'comprehend', 'vocabulari', 'inform', 'retriev', 'appear', 'import', 'new', 'field', 'understood', 'fulli', 'accept', 'basic', 'articl', 'would', 'provid', 'understand', 'variou', 'import', 'aspect', 'inform', 'storag', 'retriev']\n",
            "Query tokens: ['difficulti', 'encount', 'inform', 'retriev', 'system', 'often', 'less', 'relat', 'equip', 'use', 'failur', 'plan', 'adequ', 'document', 'analysi', 'index', 'machin', 'code', 'posit', 'programm', 'take', 'problem', 'write', 'way', 'equip', 'understand', 'articl', 'written', 'describ', 'research', 'maxim', 'effect', 'program']\n",
            "Query tokens: ['present', 'fifti', 'one', 'hundr', 'technic', 'journal', 'publish', 'averag', 'two', 'new', 'journal', 'appear', 'everi', 'day', 'mani', 'journal', 'publish', 'one', 'two', 'million', 'articl', 'appear', 'everi', 'year', 'attempt', 'made', 'cope', 'amount', 'scientif', 'technic', 'public', 'term', 'analysi', 'control', 'storag', 'retriev']\n",
            "Query tokens: ['look', 'inform', 'impact', 'autom', 'librari', 'signific', 'librari', 'gener', 'includ', 'increas', 'import', 'autom', 'view', 'prolifer', 'inform', 'today', 'autom', 'help', 'librari', 'cope', 'problem', 'autom', 'affect', 'librari', 'react', 'idea', 'autom']\n",
            "Query tokens: ['seek', 'inform', 'use', 'data', 'process', 'librari', 'mechan', 'routin', 'librari', 'process', 'procedur', 'would', 'like', 'descript', 'gener', 'specif', 'applic', 'autom', 'area', 'circul', 'catalog', 'acquisit', 'serial', 'record', 'recordkeep', 'exampl', 'base', 'oper', 'convent', 'public', 'univers', 'librari', 'practic', 'special', 'librari', 'could', 'also', 'appli', 'public', 'univers', 'librari', 'give', 'descript', 'equip', 'oper', 'present', 'project']\n",
            "Query tokens: ['resourc', 'spent', 'appli', 'inform', 'retriev', 'techniqu', 'physic', 'medic', 'scienc', 'inform', 'retriev', 'use', 'natur', 'scienc', 'social', 'scienc', 'human', 'problem', 'encount', 'subject', 'area', 'solv', 'characterist', 'subject', 'area', 'necessit', 'develop', 'new', 'inform', 'retriev', 'techniqu', 'prospct', 'futur', 'machin', 'control', 'area']\n",
            "Query tokens: ['use', 'tradit', 'classif', 'scheme', 'ddc', 'udc', 'lc', 'etc', 'inform', 'retriev', 'system', 'scheme', 'appear', 'suit', 'machin', 'use', 'appli', 'classif', 'scheme', 'irrelev', 'research', 'shown', 'subject', 'classif', 'knowledg', 'complet', 'unnecessari', 'machin', 'system', 'new', 'scheme', 'devis', 'appear', 'suit', 'machin', 'use']\n",
            "Query tokens: ['characterist', 'medlar', 'medic', 'literatur', 'analysi', 'retriev', 'system', 'project', 'undertaken', 'nation', 'librari', 'medicin', 'index', 'current', 'medic', 'journal', 'relat', 'index', 'system', 'index', 'medicu', 'major', 'compon', 'medlar', 'project', 'major', 'oper', 'detail']\n",
            "Query tokens: ['effect', 'librarian', 'autom', 'note', 'new', 'type', 'technolog', 'use', 'librari', 'effect', 'statu', 'posit', 'function', 'librarian', 'chang', 'contempl', 'initi', 'introduc', 'autom', 'educ', 'librarian']\n",
            "Query tokens: ['aim', 'object', 'medic', 'literatur', 'analysi', 'retriev', 'system', 'medlar', 'medlar', 'oper', 'possibl', 'applic', 'medlar', 'futur', 'inform', 'retriev', 'system']\n",
            "Query tokens: ['standard', 'method', 'find', 'inform', 'today', 'librari', 'use', 'alphabet', 'arrang', 'card', 'catalog', 'classifi', 'catalog', 'base', 'classif', 'system', 'dc', 'lc', 'system', 'modifi', 'use', 'autom', 'inform', 'retriev']\n",
            "Query tokens: ['catalog', 'either', 'arrang', 'alphabet', 'arrang', 'classif', 'number', 'lc', 'entri', 'print', 'readabl', 'languag', 'ultim', 'import', 'individu', 'look', 'inform', 'definit', 'author', 'titl', 'subject', 'phrase', 'languag', 'probabl', 'english', 'case', 'mind', 'lc', 'entri', 'subject', 'head', 'use', 'manner', 'autom', 'system']\n",
            "Query tokens: ['bibliograph', 'control', 'marc', 'review', 'capabl', 'key', 'onlin', 'system', 'brought', 'interdepend', 'among', 'librari', 'servic', 'center', 'mediat', 'larg', 'util', 'process', 'distribut', 'data', 'develop', 'basic', 'network', 'structur', 'among', 'librari', 'unit', 'state', 'independ', 'develop', 'major', 'network', 'brought', 'problem', 'standard', 'coordin', 'author', 'point', 'technolog', 'led', 'toward', 'central', 'autom', 'librari', 'servic', 'new', 'develop', 'push', 'toward', 'decentr', 'coordin', 'requir', 'avoid', 'fragment', 'new', 'environ', 'jasi', 'vol', '31', 'novemb', '1980', 'pp', '438444']\n",
            "Query tokens: ['way', 'individu', 'construct', 'modifi', 'search', 'queri', 'larg', 'interact', 'document', 'retriev', 'system', 'subject', 'systemat', 'bias', 'similar', 'demonstr', 'experi', 'judgement', 'uncertainti', 'bias', 'share', 'naiv', 'sophist', 'subject', 'caus', 'inquir', 'search', 'document', 'larg', 'interact', 'system', 'construct', 'modifi', 'queri', 'ineffici', 'search', 'algorithm', 'suggest', 'help', 'inquir', 'avoid', 'effect', 'bias', 'jasi', 'vol', '31', 'juli', '1980', 'pp', '271277']\n",
            "Query tokens: ['articl', 'concern', 'problem', 'permit', 'patron', 'repres', 'rel', 'import', 'variou', 'index', 'term', 'boolean', 'request', 'retain', 'desir', 'properti', 'boolean', 'system', 'charact', 'classic', 'boolean', 'system', 'review', 'relat', 'notion', 'fuzzi', 'set', 'fuzzi', 'set', 'concept', 'form', 'basi', 'concept', 'fuzzi', 'request', 'weight', 'assign', 'index', 'term', 'ther', 'properti', 'system', 'discuss', 'shown', 'system', 'retain', 'manipul', 'tradit', 'boolean', 'request', 'jasi', 'vol', '31', 'juli', '1980', 'pp', '240247']\n",
            "Query tokens: ['use', 'document', 'cluster', 'suggest', 'effici', 'file', 'organ', 'document', 'retriev', 'system', 'possibl', 'use', 'inform', 'relationship', 'document', 'effect', 'system', 'ie', 'abil', 'distinguish', 'relev', 'nonrelev', 'document', 'may', 'also', 'improv', 'paper', 'probabilist', 'model', 'cluster', 'search', 'base', 'queri', 'classif', 'describ', 'model', 'test', 'retriev', 'experi', 'indic', 'effect', 'heurist', 'cluster', 'search', 'cluster', 'search', 'base', 'model', 'also', 'effect', 'full', 'search', 'everi', 'document', 'compar', 'queri', 'effici', 'aspect', 'implement', 'model', 'discuss', 'inform', 'system', 'vol', '1980', 'pp', '189195']\n",
            "Query tokens: ['current', 'onlin', 'librari', 'network', 'technolog', 'describ', 'includ', 'physic', 'function', 'aspect', 'network', 'three', 'type', 'network', 'distinguish', 'search', 'servic', 'eg', 'sdc', 'lockhe', 'custom', 'servic', 'provid', 'bibliograph', 'file', 'eg', 'oclc', 'inc', 'rlin', 'servic', 'center', 'eg', 'nelinet', 'incolsa', 'predict', 'technolog', 'evolv', 'servic', 'provid', 'outsid', 'librari', 'directli', 'user', 'home', 'offic', 'jasi', 'vol', '31', 'novemb', '1980', 'pp', '425437']\n",
            "Query tokens: ['experiment', 'comput', 'program', 'develop', 'classifi', 'document', 'accord', '80', 'section', 'five', 'major', 'section', 'group', 'chemic', 'abstract', 'ca', 'program', 'use', 'pattern', 'recognit', 'techniqu', 'supplement', 'heurist', 'train', 'phase', 'word', 'preclassifi', 'document', 'select', 'probabl', 'occurr', 'word', 'section', 'ca', 'comput', 'store', 'refer', 'dictionari', 'classif', 'phase', 'match', 'word', 'document', 'titl', 'dictionari', 'assign', 'section', 'number', 'document', 'use', 'weight', 'deriv', 'probabl', 'dictionari', 'heurist', 'techniqu', 'use', 'normal', 'word', 'variant', 'plural', 'past', 'tens', 'gerund', 'train', 'phase', 'classif', 'phase', 'dictionari', 'lookup', 'techniqu', 'supplement', 'analysi', 'chemic', 'nomenclatur', 'term', 'compon', 'word', 'root', 'influenc', 'section', 'document', 'assign', 'program', 'perform', 'human', 'consist', 'evalu', 'compar', 'program', 'result', 'publish', 'section', 'ca', 'conduct', 'experi', 'peopl', 'experienc', 'assign', 'document', 'ca', 'section', 'program', 'assign', 'approxim', '78', 'document', 'correct', 'major', 'section', 'group', 'ca', '67', 'correct', 'section', 'crossrefer', 'rate', '100', 'document', 'per', 'second', 'jasi', 'vol', '31', 'novemb', '1980', 'pp', '396402']\n",
            "Query tokens: ['use', 'minicomput', 'variou', 'phase', 'creat', 'thesauru', 'nation', 'inform', 'center', 'special', 'educ', 'materi', 'nicsem', 'databas', 'describ', 'minicomput', 'use', 'collect', 'edit', 'correct', 'candid', 'thesauru', 'term', 'use', 'minicomput', 'eas', 'process', 'group', 'term', 'file', 'similar', 'concept', 'facilit', 'gener', 'product', 'use', 'vocabulari', 'review', 'term', 'structur', 'syndet', 'relat', 'indic', 'assign', 'code', 'identif', 'number', 'alter', 'easili', 'design', 'phase', 'reflect', 'restructur', 'requir', 'thesauru', 'term', 'alreadi', 'machin', 'readabl', 'form', 'simpl', 'prepar', 'print', 'program', 'provid', 'permut', 'alphabet', 'hierarch', 'chart', 'format', 'term', 'display', 'overal', 'use', 'minicomput', 'facilit', 'initi', 'thesauru', 'entri', 'develop', 'reduc', 'cleric', 'effort', 'editori', 'staff', 'decis', 'overal', 'process', 'time', 'jasi', 'vol', '31', 'septemb', '1980', 'pp', '363368']\n",
            "Query tokens: ['new', 'method', 'describ', 'extract', 'signific', 'phrase', 'titl', 'abstreact', 'scientif', 'technic', 'document', 'method', 'base', 'upon', 'text', 'structur', 'analysi', 'use', 'rel', 'small', 'dictionari', 'dictionari', 'construct', 'base', 'knowledg', 'concept', 'field', 'scienc', 'technolog', 'lexic', 'knowledg', 'signific', 'phrase', 'compon', 'item', 'may', 'use', 'differ', 'mean', 'among', 'field', 'text', 'analysiu', 'approach', 'appli', 'select', 'signific', 'phrase', 'substanti', 'semant', 'inform', 'carrier', 'content', 'abstract', 'result', 'experi', 'five', 'set', 'document', 'shown', 'signific', 'phrase', 'effect', 'extract', 'case', 'number', 'everi', 'document', 'process', 'time', 'fairli', 'satisfactori', 'inform', 'represent', 'document', 'partli', 'use', 'method', 'discuss', 'relat', 'construct', 'document', 'inform', 'retriev', 'system', 'info', 'proc', 'manag', 'vol', '16', '1980', 'pp119127']\n",
            "Query tokens: ['paper', 'discuss', 'origin', 'librari', 'network', 'trace', 'develop', 'unit', 'state', 'late', '1960', 'present', 'concept', 'resourc', 'share', 'particular', 'attent', 'inter', 'librari', 'loan', 'program', 'cooper', 'acquisit', 'storag', 'materi', 'examin', 'relationship', 'librari', 'network', 'particular', 'attent', 'given', 'question', 'two', 'major', 'compon', 'librari', 'cooper', 'tend', 'separ', 'might', 'becom', 'close', 'integr', 'jasi', 'vol', '31', 'novemb', '1980', 'pp', '405412']\n",
            "Query tokens: ['algorithm', 'given', 'process', 'partial', 'specifi', 'queri', 'compress', 'databas', 'system', 'propos', 'method', 'handl', 'effect', 'queri', 'use', 'either', 'whole', 'word', 'word', 'fragment', 'languag', 'element', 'method', 'compar', 'critic', 'evalu', 'term', 'design', 'retriev', 'cost', 'analys', 'show', 'method', 'exploit', 'interdepend', 'fragment', 'well', 'relev', 'fragment', 'record', 'file', 'maximum', 'design', 'cost', 'least', 'retriev', 'cost', 'inform', 'system', 'vol', 'april', '1980', 'pp', '323332']\n",
            "Query tokens: ['lexic', 'problem', 'larg', 'inform', 'system', 'creat', 'necess', 'handl', 'great', 'number', 'name', 'interrel', 'lexic', 'problem', 'cover', 'complet', 'concept', 'data', 'dictionari', 'mostli', 'concern', 'databas', 'scheme', 'design', 'rather', 'execut', 'oper', 'paper', 'introduc', 'view', 'lexic', 'subsystem', 'separ', 'compon', 'inform', 'system', 'architectur', 'deal', 'linguist', 'control', 'function', 'concern', 'lexic', 'problem', 'local', 'network', 'environ', 'lexic', 'suybsystem', 'special', 'effici', 'organ', 'program', 'packag', 'play', 'role', 'linguist', 'filter', 'broad', 'sens', 'lexic', 'incorrect', 'queri', 'promot', 'integr', 'databas', 'inform', 'retriev', 'system', 'facilit', 'creation', 'local', 'inform', 'system', 'hope', 'lexic', 'subsystem', 'becom', 'product', 'larg', 'especi', 'distribut', 'inform', 'system', 'inform', 'process', 'manag', 'vol', '16', 'februari', '1980', 'pp', '259267']\n",
            "Query tokens: ['relat', 'model', 'receiv', 'increas', 'attent', 'past', 'decad', 'advantag', 'includ', 'simplic', 'consist', 'sound', 'theoret', 'basi', 'articl', 'natur', 'view', 'inform', 'retriev', 'relat', 'demonstr', 'relat', 'model', 'present', 'relat', 'organ', 'bibliograph', 'databas', 'shown', 'notion', 'normal', 'introduc', 'first', 'second', 'third', 'fourth', 'normal', 'form', 'demonstr', 'relat', 'languag', 'discuss', 'includ', 'relat', 'calculu', 'relat', 'algebra', 'sequel', 'numer', 'exampl', 'pertin', 'inform', 'retriev', 'present', 'relat', 'languag', 'advantag', 'relat', 'approach', 'inform', 'retriev', 'note', 'jasi', 'vol', '32', 'januari', '1981', 'pp', '5164']\n",
            "Query tokens: ['techniqu', 'describ', 'automat', 'reformul', 'boolean', 'queri', 'base', 'patron', 'relev', 'judgement', 'initi', 'retriev', 'preval', 'measur', 'deriv', 'term', 'appear', 'retriev', 'set', 'document', 'reflect', 'term', 'distribut', 'among', 'relev', 'nonrelev', 'document', 'measur', 'use', 'guid', 'construct', 'boolean', 'queri', 'subsequ', 'retriev', 'illustr', 'techniqu', 'seri', 'test', 'describ', 'applic', 'small', 'data', 'base', 'experiment', 'environ', 'result', 'compar', 'favour', 'feedback', 'employ', 'smarttyp', 'system', 'extens', 'test', 'suggest', 'valid', 'techniqu', 'journal', 'document', 'vol', '36', 'septemb', '1980', 'pp', '197208']\n",
            "Query tokens: ['mani', 'inform', 'scientist', 'concern', 'oper', 'document', 'retriev', 'system', 'serv', 'scientist', 'variou', 'field', 'scientist', 'serv', 'system', 'often', 'member', 'call', 'invis', 'colleg', 'group', 'scientist', 'frequent', 'commun', 'one', 'anoth', 'involv', 'highli', 'special', 'subject', 'matter', 'often', 'group', 'consid', 'share', 'intellectu', 'perspect', 'regard', 'subject', 'matter', 'sometim', 'refer', 'paradigm', 'purpos', 'paper', 'show', 'possibl', 'identifi', 'paradigm', 'use', 'techniqu', 'citat', 'analysi', 'operation', 'notion', 'paradigm', 'consensu', 'structur', 'concept', 'field', 'suppos', 'obtain', 'set', 'paper', 'pertain', 'topic', 'alreadi', 'know', 'someth', 'field', 'read', 'text', 'mark', 'passag', 'certain', 'specif', 'concept', 'use', 'discuss', 'exampl', 'might', 'find', 'concept', 'design', 'appear', 'subset', 'paper', 'suppos', 'identifi', 'paper', 'concept', 'use', 'togeth', 'paper', 'certain', 'specifi', 'manner', 'clearli', 'concept', 'combin', 'natur', 'way', 'author', 'combin', 'concept', 'way', 'though', 'predomin', 'mode', 'may', 'emerg', 'set', 'concept', 'structur', 'given', 'total', 'admiss', 'combin', 'concept', 'taken', 'two', 'time', 'frequenc', 'given', 'combin', 'occur', 'sampl', 'paper', 'topic', 'measur', 'degre', 'consensu', 'regard', 'particular', 'concept', 'combin', 'within', 'corpu', 'concept', 'taken', 'two', 'time', 'structur', 'display', 'graph', 'concept', 'node', 'relat', 'repres', 'line', 'arc', 'connect', 'node', 'definit', 'concept', 'structur', 'similar', 'semant', 'network', 'artifici', 'intellig', 'except', 'approach', 'measur', 'consensu', 'weight', 'arc', 'graph', 'journal', 'document', 'vol', '36', 'septemb', '1980', 'pp', '183196']\n",
            "Query tokens: ['number', 'databas', 'record', 'contain', 'databas', 'onlin', 'use', 'databas', 'increas', 'dramat', 'past', 'sever', 'year', 'bring', '1979', 'total', 'bibliograph', 'bioliographicrel', 'natur', 'languag', 'databas', '528', '528', 'databas', 'contain', '148', 'million', 'record', 'million', 'onlin', 'search', 'conduct', 'via', 'major', 'us', 'canadian', 'system', '1979', 'bulletin', 'asi', 'vol', 'decemb', '1980', 'pp', '2729']\n",
            "Query tokens: ['major', 'defici', 'tradit', 'boolean', 'system', 'inabl', 'repres', 'vari', 'degre', 'document', 'may', 'written', 'subject', 'articl', 'isol', 'number', 'criteria', 'met', 'boolean', 'system', 'gener', 'weight', 'capabl', 'proven', 'one', 'weight', 'rule', 'satisfi', 'conditionsthat', 'associ', 'fuzzi', 'set', 'theoryand', 'weight', 'scheme', 'satisfi', 'properti', 'associ', 'boolean', 'algebra', 'well', 'probabilist', 'weight', 'introduc', 'altern', 'approach', 'two', 'system', 'compar', 'limit', 'zeroon', 'weight', 'system', 'consid', 'converg', 'tradit', 'boolean', 'retriev', 'jasi', 'vol', '32', 'juli', '1981']\n",
            "Query tokens: ['sever', 'paper', 'appear', 'analyz', 'recent', 'develop', 'problem', 'process', 'document', 'retriev', 'system', 'queri', 'express', 'boolean', 'express', 'purpos', 'paper', 'continu', 'analysi', 'shall', 'show', 'concept', 'threshold', 'valu', 'resolv', 'problem', 'inher', 'relev', 'weight', 'moreov', 'shall', 'explor', 'possibl', 'evalu', 'mechan', 'retriev', 'document', 'base', 'fuzzysettheoret', 'consider', 'inform', 'process', 'manag', 'vol', '17', '1981', 'pp', '127136']\n",
            "Query tokens: ['good', 'deal', 'work', 'inform', 'retriev', 'system', 'continu', 'weight', 'assign', 'index', 'term', 'describ', 'record', 'databas', 'andor', 'queri', 'term', 'describ', 'user', 'queri', 'recent', 'articl', 'analyz', 'retriev', 'system', 'continu', 'weight', 'either', 'type', 'andor', 'boolean', 'structur', 'queri', 'also', 'suggest', 'criteria', 'system', 'ought', 'satisfi', 'record', 'evalu', 'mechan', 'partial', 'satisfi', 'criteria', 'offer', 'care', 'analysi', 'base', 'gener', 'discret', 'weight', 'also', 'look', 'weight', 'entir', 'differ', 'approach', 'involv', 'threshold', 'gener', 'improv', 'evalu', 'mechan', 'seem', 'fulfil', 'larger', 'subset', 'desir', 'criteria', 'previou', 'mechan', 'new', 'mechan', 'allow', 'user', 'attach', 'threshold', 'queri', 'term', 'jasi', 'vol', '32', 'may', '1981', 'pp', '211216']\n",
            "Query tokens: ['onlin', 'retriev', 'system', 'may', 'difficult', 'use', 'especi', 'end', 'user', 'heterogen', 'complex', 'investig', 'concern', 'concept', 'translat', 'comput', 'interfac', 'mean', 'simplifi', 'access', 'oper', 'heterogen', 'bibliograph', 'retriev', 'system', 'databas', 'interfac', 'allow', 'user', 'make', 'request', 'common', 'languag', 'request', 'translat', 'interfac', 'appropri', 'command', 'whatev', 'system', 'interrog', 'system', 'respons', 'may', 'also', 'transform', 'interfac', 'common', 'form', 'given', 'user', 'thu', 'network', 'differ', 'system', 'made', 'look', 'like', 'singl', 'virtual', 'system', 'user', 'interfac', 'also', 'provid', 'instruct', 'search', 'aid', 'user', 'philosophi', 'design', 'implement', 'experiment', 'interfac', 'name', 'conit', 'describ', 'jasi', 'vol', '32', 'juli', '1981', 'pp', '287303']\n",
            "Query tokens: ['evalu', 'concept', 'translat', 'compuyt', 'interfac', 'simplifi', 'oper', 'multipl', 'heterogen', 'onlin', 'bibliograph', 'retriev', 'system', 'undertaken', 'experiment', 'retriev', 'system', 'name', 'conit', 'built', 'test', 'control', 'condit', 'inexperienc', 'end', 'user', 'detail', 'analysi', 'experiment', 'usag', 'show', 'user', 'abl', 'master', 'interfac', 'oper', 'suffici', 'well', 'find', 'relev', 'document', 'refer', 'success', 'attribut', 'part', 'simpl', 'command', 'languag', 'adequ', 'onlin', 'instruct', 'simplifi', 'naturallanguag', 'keywordstem', 'approach', 'search', 'conclud', 'oper', 'interfac', 'type', 'studi', 'provid', 'increas', 'usabl', 'exist', 'system', 'cost', 'effect', 'manner', 'especi', 'searcher', 'furthermor', 'advanc', 'interfac', 'base', 'improv', 'instruct', 'autom', 'search', 'strategi', 'techniqu', 'could', 'enhanc', 'retriev', 'effect', 'wide', 'class', 'user', 'jasi', 'vol', '32', 'juli', '1981', 'pp', '304317']\n",
            "Query tokens: ['paper', 'note', 'benefit', 'accru', 'interact', 'computer', 'retriev', 'system', 'micrograph', 'retriev', 'system', 'review', 'current', 'state', 'autom', 'micrograph', 'retriev', 'technolog', 'conclus', 'combin', 'advanc', 'commun', 'technolog', 'sophist', 'index', 'input', 'librari', 'inform', 'scientist', 'new', 'gener', 'autom', 'micrograph', 'devic', 'may', 'constitut', 'onlin', 'document', 'retriev', 'system', 'futur', 'journal', 'inform', 'scienc', '1980', 'pp', '345349']\n",
            "  Evaluated 100 queries...\n",
            "Query tokens: ['convent', 'inform', 'retriev', 'process', 'larg', 'base', 'data', 'movement', 'pointer', 'manipul', 'integ', 'arithmet', 'refin', 'retriev', 'algorithm', 'may', 'addit', 'benefit', 'substanti', 'comput', 'power', 'present', 'studi', 'number', 'parallel', 'process', 'method', 'describ', 'serv', 'enhanc', 'retriev', 'servic', 'convent', 'retriev', 'environ', 'parallel', 'list', 'process', 'parallel', 'search', 'facil', 'greatest', 'interest', 'advanc', 'system', 'use', 'array', 'processor', 'also', 'prove', 'benefici', 'variou', 'inform', 'retriev', 'process', 'examin', 'evid', 'given', 'demonstr', 'use', 'parallel', 'process', 'fast', 'comput', 'facil', 'inform', 'retriev', 'lectur', 'note', 'comput', 'scienc', 'iii', 'handler', 'ed', 'springer', 'verlag', 'berlinnew', 'york', '1981', 'pp', '328342']\n",
            "Query tokens: ['frequenc', 'characterist', 'term', 'document', 'collect', 'use', 'indic', 'term', 'import', 'content', 'analysi', 'index', 'purpos', 'particular', 'rare', 'frequent', 'term', 'normal', 'believ', 'less', 'effect', 'mediumfrequ', 'term', 'recent', 'automat', 'index', 'theori', 'devis', 'use', 'term', 'frequenc', 'characterist', 'also', 'relev', 'properti', 'term', 'major', 'termweight', 'theori', 'first', 'briefli', 'review', 'term', 'precis', 'term', 'util', 'weight', 'base', 'occurr', 'characterist', 'term', 'relev', 'oppos', 'nonrelev', 'document', 'collect', 'introduc', 'method', 'suggest', 'estim', 'relev', 'properti', 'term', 'base', 'overal', 'occurr', 'characterist', 'collect', 'final', 'experiment', 'evalu', 'result', 'shown', 'compar', 'weight', 'system', 'use', 'term', 'relev', 'properti', 'convent', 'frequencybas', 'methodolog', 'jasi', 'vol', '32', 'may', '1981', 'pp', '175186']\n",
            "Query tokens: ['paper', 'tackl', 'problem', 'one', 'might', 'select', 'search', 'term', 'use', 'relev', 'feedback', 'given', 'search', 'term', 'queri', 'search', 'term', 'extract', 'maximum', 'span', 'tree', 'connect', 'term', 'index', 'term', 'vocabulari', 'number', 'differ', 'span', 'tree', 'gener', 'varieti', 'associ', 'measur', 'retriev', 'effect', 'differ', 'span', 'tree', 'shown', 'approxim', 'effect', 'measur', 'term', 'precis', 'recal', 'retriev', 'test', 'done', 'three', 'differ', 'test', 'collect', 'inform', 'process', 'manag', 'vol', '17', '1981', 'pp', '7791']\n",
            "Query tokens: ['shown', 'map', 'particular', 'area', 'scienc', 'case', 'inform', 'scienc', 'done', 'use', 'author', 'unit', 'analysi', 'cocit', 'pair', 'author', 'variabl', 'indic', 'distanc', 'analysi', 'assum', 'two', 'author', 'cite', 'togeth', 'closer', 'relationship', 'raw', 'data', 'cocit', 'count', 'drawn', 'onlin', 'social', 'scisearch', 'social', 'scienc', 'citat', 'index', 'period', '19721979', 'gthe', 'result', 'map', 'show', 'identifi', 'author', 'group', 'akin', 'school', 'inform', 'scienc', 'locat', 'group', 'respect', 'degre', 'central', 'peripher', 'author', 'within', 'group', 'proxim', 'author', 'within', 'group', 'across', 'group', 'boundari', 'border', 'author', 'seem', 'connect', 'variou', 'area', 'research', 'posit', 'author', 'respect', 'map', 'axe', 'arbitrarili', 'set', 'span', 'diverg', 'group', 'order', 'aid', 'interpret', 'cocit', 'analysi', 'author', 'offer', 'new', 'techniqu', 'might', 'contribut', 'understand', 'intellectu', 'structur', 'scienc', 'possibl', 'area', 'extent', 'area', 'reli', 'serial', 'public', 'techniqu', 'establish', 'author', 'well', 'document', 'effect', 'unit', 'analyz', 'subject', 'specialti', 'jasi', 'vol', '32', 'may', '1981', 'pp', '163171']\n",
            "Query tokens: ['autom', 'document', 'cluster', 'procedur', 'describ', 'requir', 'use', 'interdocu', 'similar', 'matrix', 'independ', 'order', 'document', 'process', 'procedur', 'make', 'use', 'initi', 'set', 'cluster', 'deriv', 'certain', 'term', 'index', 'vocabulari', 'use', 'characteris', 'document', 'file', 'retriev', 'effect', 'obtain', 'use', 'cluster', 'file', 'compar', 'obtain', 'serial', 'search', 'use', 'singlelinkag', 'cluster', 'method', 'journal', 'inform', 'scienc', '1980', 'pp', '222231']\n",
            "\n",
            " MAP@10: 0.0673\n",
            "   Evaluated 76 queries\n",
            "   Best AP: 1.0000\n",
            "   Worst AP: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**nDCG (Normalized Discounted Cumulative Gain)**"
      ],
      "metadata": {
        "id": "TdSW5JRWEGyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "print(\"\\nSTEP 5: CALCULATING nDCG\")\n",
        "\n",
        "def dcg_at_k(retrieved, relevant, k=10):\n",
        "    \"\"\"\n",
        "    Calculate DCG@K\n",
        "\n",
        "    Args:\n",
        "        retrieved: List of retrieved document IDs\n",
        "        relevant: List of relevant document IDs\n",
        "        k: Number of results to consider\n",
        "\n",
        "    Returns:\n",
        "        float: DCG score\n",
        "    \"\"\"\n",
        "    relevant_set = set(relevant)\n",
        "    dcg = 0\n",
        "\n",
        "    for i, doc_id in enumerate(retrieved[:k], 1):\n",
        "        if doc_id in relevant_set:\n",
        "\n",
        "            rel = 1\n",
        "            dcg += rel / math.log2(i + 1)\n",
        "\n",
        "    return dcg\n",
        "\n",
        "def ndcg_at_k(retrieved, relevant, k=10):\n",
        "    \"\"\"\n",
        "    Calculate nDCG@K\n",
        "\n",
        "    Args:\n",
        "        retrieved: List of retrieved document IDs\n",
        "        relevant: List of relevant document IDs\n",
        "        k: Number of results to consider\n",
        "\n",
        "    Returns:\n",
        "        float: nDCG score\n",
        "    \"\"\"\n",
        "\n",
        "    dcg = dcg_at_k(retrieved, relevant, k)\n",
        "\n",
        "\n",
        "    ideal_retrieved = relevant[:k]\n",
        "    idcg = dcg_at_k(ideal_retrieved, relevant, k)\n",
        "\n",
        "    if idcg == 0:\n",
        "        return 0\n",
        "\n",
        "    ndcg = dcg / idcg\n",
        "    return ndcg\n",
        "\n",
        "\n",
        "query_id = 1\n",
        "results = simple_search(queries[query_id]['text'], top_k=10)\n",
        "retrieved = [doc_id for doc_id, score in results]\n",
        "relevant = relevance[query_id]\n",
        "\n",
        "ndcg = ndcg_at_k(retrieved, relevant, k=10)\n",
        "\n",
        "print(f\"\\nQuery {query_id}\")\n",
        "print(f\" nDCG@10: {ndcg:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ChFEmf_EEhc",
        "outputId": "9f658f92-8165-49dc-b49a-84956bcca2d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STEP 5: CALCULATING nDCG\n",
            "Query tokens: ['problem', 'concern', 'make', 'descript', 'titl', 'difficulti', 'involv', 'automat', 'retriev', 'articl', 'approxim', 'titl', 'usual', 'relev', 'content', 'articl', 'titl']\n",
            "\n",
            "Query 1\n",
            " nDCG@10: 0.5068\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate ALL Queries (Complete Report)**"
      ],
      "metadata": {
        "id": "QW2aKHT7EUFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 6: COMPLETE EVALUATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def evaluate_all_queries(queries_dict, relevance_dict, top_k=10):\n",
        "    \"\"\"Complete evaluation with all metrics\"\"\"\n",
        "\n",
        "    results = {\n",
        "        'precision': [],\n",
        "        'recall': [],\n",
        "        'ap': [],\n",
        "        'ndcg': []\n",
        "    }\n",
        "\n",
        "    print(\"\\nEvaluating all queries...\")\n",
        "\n",
        "    for query_id in relevance_dict.keys():\n",
        "        if query_id not in queries_dict:\n",
        "            continue\n",
        "\n",
        "        # Search\n",
        "        query_text = queries_dict[query_id]['text']\n",
        "        search_results = simple_search(query_text, top_k=top_k)\n",
        "        retrieved = [doc_id for doc_id, score in search_results]\n",
        "        relevant = relevance_dict[query_id]\n",
        "\n",
        "        # Calculate metrics\n",
        "        p = precision_at_k(retrieved, relevant, k=top_k)\n",
        "        r = recall_at_k(retrieved, relevant, k=top_k)\n",
        "        ap = average_precision(retrieved, relevant)\n",
        "        ndcg = ndcg_at_k(retrieved, relevant, k=top_k)\n",
        "\n",
        "        results['precision'].append(p)\n",
        "        results['recall'].append(r)\n",
        "        results['ap'].append(ap)\n",
        "        results['ndcg'].append(ndcg)\n",
        "\n",
        "        if query_id % 20 == 0:\n",
        "            print(f\"  Processed {query_id} queries...\")\n",
        "\n",
        "    # Calculate averages\n",
        "    avg_results = {\n",
        "        'Precision@10': sum(results['precision']) / len(results['precision']),\n",
        "        'Recall@10': sum(results['recall']) / len(results['recall']),\n",
        "        'MAP@10': sum(results['ap']) / len(results['ap']),\n",
        "        'nDCG@10': sum(results['ndcg']) / len(results['ndcg'])\n",
        "    }\n",
        "\n",
        "    return avg_results, results\n",
        "\n",
        "# Run complete evaluation\n",
        "avg_results, detailed_results = evaluate_all_queries(queries, relevance, top_k=10)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"📊 FINAL EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total Queries Evaluated: {len(detailed_results['precision'])}\")\n",
        "print()\n",
        "for metric, score in avg_results.items():\n",
        "    print(f\"{metric}: {score:.4f} ({score*100:.2f}%)\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "C6t_V7TlEYVW",
        "outputId": "980a2d88-0633-4ac3-c4ec-9456e72e5ce5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 6: COMPLETE EVALUATION REPORT\n",
            "============================================================\n",
            "\n",
            "Evaluating all queries...\n",
            "Query tokens: ['problem', 'concern', 'make', 'descript', 'titl', 'difficulti', 'involv', 'automat', 'retriev', 'articl', 'approxim', 'titl', 'usual', 'relev', 'content', 'articl', 'titl']\n",
            "Query tokens: ['actual', 'pertin', 'data', 'oppos', 'refer', 'entir', 'articl', 'retriev', 'automat', 'respons', 'inform', 'request']\n",
            "Query tokens: ['inform', 'scienc', 'give', 'definit', 'possibl']\n",
            "Query tokens: ['imag', 'recognit', 'method', 'automat', 'transform', 'print', 'text', 'computerreadi', 'form']\n",
            "Query tokens: ['special', 'train', 'ordinari', 'research', 'businessmen', 'need', 'proper', 'inform', 'manag', 'unobstruct', 'use', 'inform', 'retriev', 'system', 'problem', 'like', 'encount']\n",
            "Query tokens: ['possibl', 'verbal', 'commun', 'comput', 'human', 'commun', 'via', 'spoken', 'word']\n",
            "Query tokens: ['describ', 'present', 'work', 'plan', 'system', 'publish', 'print', 'origin', 'paper', 'comput', 'save', 'byproduct', 'articl', 'code', 'dataprocess', 'form', 'use', 'retriev']\n",
            "Query tokens: ['describ', 'inform', 'retriev', 'index', 'languag', 'bear', 'scienc', 'gener']\n",
            "Query tokens: ['possibl', 'automat', 'grammat', 'contextu', 'analysi', 'articl', 'inclus', 'inform', 'retriev', 'system']\n",
            "Query tokens: ['use', 'abstract', 'mathemat', 'inform', 'retriev', 'eg', 'group', 'theori']\n",
            "Query tokens: ['need', 'inform', 'consolid', 'evalu', 'retriev', 'scientif', 'research']\n",
            "Query tokens: ['give', 'method', 'high', 'speed', 'public', 'print', 'distribut', 'scientif', 'journal']\n",
            "Query tokens: ['criteria', 'develop', 'object', 'evalu', 'inform', 'retriev', 'dissemin', 'system']\n",
            "Query tokens: ['futur', 'automat', 'medic', 'diagnosi']\n",
            "Query tokens: ['much', 'inform', 'retriev', 'dissemin', 'system', 'well', 'autom', 'librari', 'cost', 'worth', 'research', 'industri']\n",
            "Query tokens: ['system', 'incorpor', 'multiprogram', 'remot', 'station', 'inform', 'retriev', 'extent', 'use', 'futur']\n",
            "Query tokens: ['mean', 'obtain', 'larg', 'volum', 'high', 'speed', 'custom', 'usabl', 'inform', 'retriev', 'output']\n",
            "Query tokens: ['method', 'encod', 'automat', 'match', 'automat', 'draw', 'structur', 'extend', 'two', 'dimens', 'like', 'structur', 'formula', 'chemic', 'compound']\n",
            "Query tokens: ['techniqu', 'machin', 'match', 'machin', 'search', 'system', 'code', 'match', 'method']\n",
            "Query tokens: ['test', 'autom', 'inform', 'system']\n",
            "  Processed 20 queries...\n",
            "Query tokens: ['need', 'provid', 'personnel', 'inform', 'field']\n",
            "Query tokens: ['autom', 'inform', 'medic', 'field']\n",
            "Query tokens: ['amount', 'use', 'book', 'librari', 'relat', 'need', 'autom', 'inform', 'system']\n",
            "Query tokens: ['educ', 'train', 'requir', 'personnel', 'inform', 'field', 'possibl', 'train', 'need', 'program', 'provid', 'train']\n",
            "Query tokens: ['intern', 'system', 'exchang', 'dissemin', 'inform']\n",
            "Query tokens: ['cost', 'determin', 'cost', 'associ', 'system', 'autom', 'inform']\n",
            "Query tokens: ['computer', 'inform', 'retriev', 'system', 'computer', 'index', 'system']\n",
            "Query tokens: ['computer', 'inform', 'system', 'field', 'relat', 'chemistri']\n",
            "Query tokens: ['specif', 'advantag', 'computer', 'index', 'system']\n",
            "Query tokens: ['inform', 'dissemin', 'journal', 'period']\n",
            "Query tokens: ['inform', 'system', 'physic', 'scienc']\n",
            "Query tokens: ['attempt', 'computer', 'mechan', 'system', 'gener', 'librari', 'problem', 'method', 'autom', 'gener', 'author', 'titl', 'index', 'system']\n",
            "Query tokens: ['retriev', 'system', 'provid', 'autom', 'transmiss', 'inform', 'user', 'distanc']\n",
            "Query tokens: ['method', 'code', 'use', 'computer', 'index', 'system']\n",
            "Query tokens: ['govern', 'support', 'agenc', 'project', 'deal', 'inform', 'dissemin']\n",
            "Query tokens: ['list', 'word', 'use', 'index', 'classifi', 'materi', 'avail', 'want', 'list', 'term', 'descript', 'vocabulari', 'particular', 'field', 'schedul', 'word', 'relat', 'meaning', 'scheme', 'want', 'list', 'test', 'least', 'extent', 'found', 'use', 'organ', 'materi', 'retriev']\n",
            "Query tokens: ['progress', 'inform', 'retriev', 'present', 'problem', 'maladjust', 'disloc', 'personnel', 'train', 'retrain', 'peopl', 'use', 'new', 'equip', 'import', 'level', 'librarian', 'assist', 'technician', 'student', 'research', 'even', 'execut', 'need', 'educ', 'learn', 'purpos', 'valu', 'use', 'inform', 'system', 'hardwar', 'program', 'develop', 'chang', 'attitud', 'skill', 'tradit', 'worker', 'help', 'learn', 'newer', 'techniqu']\n",
            "Query tokens: ['alphabet', 'order', 'materi', 'consid', 'use', 'tool', 'inform', 'retriev', 'studi', 'done', 'compar', 'effect', 'alphabet', 'order', 'organ', 'scheme', 'gener', 'accept', 'form', 'arrang', 'materi', 'alphabet', 'order', 'easi', 'way', 'achiev', 'form', 'without', 'go', 'great', 'amount', 'effort']\n",
            "Query tokens: ['averag', 'student', 'research', 'difficulti', 'comprehend', 'vocabulari', 'inform', 'retriev', 'appear', 'import', 'new', 'field', 'understood', 'fulli', 'accept', 'basic', 'articl', 'would', 'provid', 'understand', 'variou', 'import', 'aspect', 'inform', 'storag', 'retriev']\n",
            "Query tokens: ['difficulti', 'encount', 'inform', 'retriev', 'system', 'often', 'less', 'relat', 'equip', 'use', 'failur', 'plan', 'adequ', 'document', 'analysi', 'index', 'machin', 'code', 'posit', 'programm', 'take', 'problem', 'write', 'way', 'equip', 'understand', 'articl', 'written', 'describ', 'research', 'maxim', 'effect', 'program']\n",
            "Query tokens: ['present', 'fifti', 'one', 'hundr', 'technic', 'journal', 'publish', 'averag', 'two', 'new', 'journal', 'appear', 'everi', 'day', 'mani', 'journal', 'publish', 'one', 'two', 'million', 'articl', 'appear', 'everi', 'year', 'attempt', 'made', 'cope', 'amount', 'scientif', 'technic', 'public', 'term', 'analysi', 'control', 'storag', 'retriev']\n",
            "Query tokens: ['look', 'inform', 'impact', 'autom', 'librari', 'signific', 'librari', 'gener', 'includ', 'increas', 'import', 'autom', 'view', 'prolifer', 'inform', 'today', 'autom', 'help', 'librari', 'cope', 'problem', 'autom', 'affect', 'librari', 'react', 'idea', 'autom']\n",
            "Query tokens: ['seek', 'inform', 'use', 'data', 'process', 'librari', 'mechan', 'routin', 'librari', 'process', 'procedur', 'would', 'like', 'descript', 'gener', 'specif', 'applic', 'autom', 'area', 'circul', 'catalog', 'acquisit', 'serial', 'record', 'recordkeep', 'exampl', 'base', 'oper', 'convent', 'public', 'univers', 'librari', 'practic', 'special', 'librari', 'could', 'also', 'appli', 'public', 'univers', 'librari', 'give', 'descript', 'equip', 'oper', 'present', 'project']\n",
            "Query tokens: ['resourc', 'spent', 'appli', 'inform', 'retriev', 'techniqu', 'physic', 'medic', 'scienc', 'inform', 'retriev', 'use', 'natur', 'scienc', 'social', 'scienc', 'human', 'problem', 'encount', 'subject', 'area', 'solv', 'characterist', 'subject', 'area', 'necessit', 'develop', 'new', 'inform', 'retriev', 'techniqu', 'prospct', 'futur', 'machin', 'control', 'area']\n",
            "Query tokens: ['use', 'tradit', 'classif', 'scheme', 'ddc', 'udc', 'lc', 'etc', 'inform', 'retriev', 'system', 'scheme', 'appear', 'suit', 'machin', 'use', 'appli', 'classif', 'scheme', 'irrelev', 'research', 'shown', 'subject', 'classif', 'knowledg', 'complet', 'unnecessari', 'machin', 'system', 'new', 'scheme', 'devis', 'appear', 'suit', 'machin', 'use']\n",
            "Query tokens: ['characterist', 'medlar', 'medic', 'literatur', 'analysi', 'retriev', 'system', 'project', 'undertaken', 'nation', 'librari', 'medicin', 'index', 'current', 'medic', 'journal', 'relat', 'index', 'system', 'index', 'medicu', 'major', 'compon', 'medlar', 'project', 'major', 'oper', 'detail']\n",
            "Query tokens: ['effect', 'librarian', 'autom', 'note', 'new', 'type', 'technolog', 'use', 'librari', 'effect', 'statu', 'posit', 'function', 'librarian', 'chang', 'contempl', 'initi', 'introduc', 'autom', 'educ', 'librarian']\n",
            "Query tokens: ['aim', 'object', 'medic', 'literatur', 'analysi', 'retriev', 'system', 'medlar', 'medlar', 'oper', 'possibl', 'applic', 'medlar', 'futur', 'inform', 'retriev', 'system']\n",
            "Query tokens: ['standard', 'method', 'find', 'inform', 'today', 'librari', 'use', 'alphabet', 'arrang', 'card', 'catalog', 'classifi', 'catalog', 'base', 'classif', 'system', 'dc', 'lc', 'system', 'modifi', 'use', 'autom', 'inform', 'retriev']\n",
            "Query tokens: ['catalog', 'either', 'arrang', 'alphabet', 'arrang', 'classif', 'number', 'lc', 'entri', 'print', 'readabl', 'languag', 'ultim', 'import', 'individu', 'look', 'inform', 'definit', 'author', 'titl', 'subject', 'phrase', 'languag', 'probabl', 'english', 'case', 'mind', 'lc', 'entri', 'subject', 'head', 'use', 'manner', 'autom', 'system']\n",
            "Query tokens: ['bibliograph', 'control', 'marc', 'review', 'capabl', 'key', 'onlin', 'system', 'brought', 'interdepend', 'among', 'librari', 'servic', 'center', 'mediat', 'larg', 'util', 'process', 'distribut', 'data', 'develop', 'basic', 'network', 'structur', 'among', 'librari', 'unit', 'state', 'independ', 'develop', 'major', 'network', 'brought', 'problem', 'standard', 'coordin', 'author', 'point', 'technolog', 'led', 'toward', 'central', 'autom', 'librari', 'servic', 'new', 'develop', 'push', 'toward', 'decentr', 'coordin', 'requir', 'avoid', 'fragment', 'new', 'environ', 'jasi', 'vol', '31', 'novemb', '1980', 'pp', '438444']\n",
            "Query tokens: ['way', 'individu', 'construct', 'modifi', 'search', 'queri', 'larg', 'interact', 'document', 'retriev', 'system', 'subject', 'systemat', 'bias', 'similar', 'demonstr', 'experi', 'judgement', 'uncertainti', 'bias', 'share', 'naiv', 'sophist', 'subject', 'caus', 'inquir', 'search', 'document', 'larg', 'interact', 'system', 'construct', 'modifi', 'queri', 'ineffici', 'search', 'algorithm', 'suggest', 'help', 'inquir', 'avoid', 'effect', 'bias', 'jasi', 'vol', '31', 'juli', '1980', 'pp', '271277']\n",
            "Query tokens: ['articl', 'concern', 'problem', 'permit', 'patron', 'repres', 'rel', 'import', 'variou', 'index', 'term', 'boolean', 'request', 'retain', 'desir', 'properti', 'boolean', 'system', 'charact', 'classic', 'boolean', 'system', 'review', 'relat', 'notion', 'fuzzi', 'set', 'fuzzi', 'set', 'concept', 'form', 'basi', 'concept', 'fuzzi', 'request', 'weight', 'assign', 'index', 'term', 'ther', 'properti', 'system', 'discuss', 'shown', 'system', 'retain', 'manipul', 'tradit', 'boolean', 'request', 'jasi', 'vol', '31', 'juli', '1980', 'pp', '240247']\n",
            "Query tokens: ['use', 'document', 'cluster', 'suggest', 'effici', 'file', 'organ', 'document', 'retriev', 'system', 'possibl', 'use', 'inform', 'relationship', 'document', 'effect', 'system', 'ie', 'abil', 'distinguish', 'relev', 'nonrelev', 'document', 'may', 'also', 'improv', 'paper', 'probabilist', 'model', 'cluster', 'search', 'base', 'queri', 'classif', 'describ', 'model', 'test', 'retriev', 'experi', 'indic', 'effect', 'heurist', 'cluster', 'search', 'cluster', 'search', 'base', 'model', 'also', 'effect', 'full', 'search', 'everi', 'document', 'compar', 'queri', 'effici', 'aspect', 'implement', 'model', 'discuss', 'inform', 'system', 'vol', '1980', 'pp', '189195']\n",
            "Query tokens: ['current', 'onlin', 'librari', 'network', 'technolog', 'describ', 'includ', 'physic', 'function', 'aspect', 'network', 'three', 'type', 'network', 'distinguish', 'search', 'servic', 'eg', 'sdc', 'lockhe', 'custom', 'servic', 'provid', 'bibliograph', 'file', 'eg', 'oclc', 'inc', 'rlin', 'servic', 'center', 'eg', 'nelinet', 'incolsa', 'predict', 'technolog', 'evolv', 'servic', 'provid', 'outsid', 'librari', 'directli', 'user', 'home', 'offic', 'jasi', 'vol', '31', 'novemb', '1980', 'pp', '425437']\n",
            "Query tokens: ['experiment', 'comput', 'program', 'develop', 'classifi', 'document', 'accord', '80', 'section', 'five', 'major', 'section', 'group', 'chemic', 'abstract', 'ca', 'program', 'use', 'pattern', 'recognit', 'techniqu', 'supplement', 'heurist', 'train', 'phase', 'word', 'preclassifi', 'document', 'select', 'probabl', 'occurr', 'word', 'section', 'ca', 'comput', 'store', 'refer', 'dictionari', 'classif', 'phase', 'match', 'word', 'document', 'titl', 'dictionari', 'assign', 'section', 'number', 'document', 'use', 'weight', 'deriv', 'probabl', 'dictionari', 'heurist', 'techniqu', 'use', 'normal', 'word', 'variant', 'plural', 'past', 'tens', 'gerund', 'train', 'phase', 'classif', 'phase', 'dictionari', 'lookup', 'techniqu', 'supplement', 'analysi', 'chemic', 'nomenclatur', 'term', 'compon', 'word', 'root', 'influenc', 'section', 'document', 'assign', 'program', 'perform', 'human', 'consist', 'evalu', 'compar', 'program', 'result', 'publish', 'section', 'ca', 'conduct', 'experi', 'peopl', 'experienc', 'assign', 'document', 'ca', 'section', 'program', 'assign', 'approxim', '78', 'document', 'correct', 'major', 'section', 'group', 'ca', '67', 'correct', 'section', 'crossrefer', 'rate', '100', 'document', 'per', 'second', 'jasi', 'vol', '31', 'novemb', '1980', 'pp', '396402']\n",
            "Query tokens: ['use', 'minicomput', 'variou', 'phase', 'creat', 'thesauru', 'nation', 'inform', 'center', 'special', 'educ', 'materi', 'nicsem', 'databas', 'describ', 'minicomput', 'use', 'collect', 'edit', 'correct', 'candid', 'thesauru', 'term', 'use', 'minicomput', 'eas', 'process', 'group', 'term', 'file', 'similar', 'concept', 'facilit', 'gener', 'product', 'use', 'vocabulari', 'review', 'term', 'structur', 'syndet', 'relat', 'indic', 'assign', 'code', 'identif', 'number', 'alter', 'easili', 'design', 'phase', 'reflect', 'restructur', 'requir', 'thesauru', 'term', 'alreadi', 'machin', 'readabl', 'form', 'simpl', 'prepar', 'print', 'program', 'provid', 'permut', 'alphabet', 'hierarch', 'chart', 'format', 'term', 'display', 'overal', 'use', 'minicomput', 'facilit', 'initi', 'thesauru', 'entri', 'develop', 'reduc', 'cleric', 'effort', 'editori', 'staff', 'decis', 'overal', 'process', 'time', 'jasi', 'vol', '31', 'septemb', '1980', 'pp', '363368']\n",
            "Query tokens: ['new', 'method', 'describ', 'extract', 'signific', 'phrase', 'titl', 'abstreact', 'scientif', 'technic', 'document', 'method', 'base', 'upon', 'text', 'structur', 'analysi', 'use', 'rel', 'small', 'dictionari', 'dictionari', 'construct', 'base', 'knowledg', 'concept', 'field', 'scienc', 'technolog', 'lexic', 'knowledg', 'signific', 'phrase', 'compon', 'item', 'may', 'use', 'differ', 'mean', 'among', 'field', 'text', 'analysiu', 'approach', 'appli', 'select', 'signific', 'phrase', 'substanti', 'semant', 'inform', 'carrier', 'content', 'abstract', 'result', 'experi', 'five', 'set', 'document', 'shown', 'signific', 'phrase', 'effect', 'extract', 'case', 'number', 'everi', 'document', 'process', 'time', 'fairli', 'satisfactori', 'inform', 'represent', 'document', 'partli', 'use', 'method', 'discuss', 'relat', 'construct', 'document', 'inform', 'retriev', 'system', 'info', 'proc', 'manag', 'vol', '16', '1980', 'pp119127']\n",
            "Query tokens: ['paper', 'discuss', 'origin', 'librari', 'network', 'trace', 'develop', 'unit', 'state', 'late', '1960', 'present', 'concept', 'resourc', 'share', 'particular', 'attent', 'inter', 'librari', 'loan', 'program', 'cooper', 'acquisit', 'storag', 'materi', 'examin', 'relationship', 'librari', 'network', 'particular', 'attent', 'given', 'question', 'two', 'major', 'compon', 'librari', 'cooper', 'tend', 'separ', 'might', 'becom', 'close', 'integr', 'jasi', 'vol', '31', 'novemb', '1980', 'pp', '405412']\n",
            "Query tokens: ['algorithm', 'given', 'process', 'partial', 'specifi', 'queri', 'compress', 'databas', 'system', 'propos', 'method', 'handl', 'effect', 'queri', 'use', 'either', 'whole', 'word', 'word', 'fragment', 'languag', 'element', 'method', 'compar', 'critic', 'evalu', 'term', 'design', 'retriev', 'cost', 'analys', 'show', 'method', 'exploit', 'interdepend', 'fragment', 'well', 'relev', 'fragment', 'record', 'file', 'maximum', 'design', 'cost', 'least', 'retriev', 'cost', 'inform', 'system', 'vol', 'april', '1980', 'pp', '323332']\n",
            "Query tokens: ['lexic', 'problem', 'larg', 'inform', 'system', 'creat', 'necess', 'handl', 'great', 'number', 'name', 'interrel', 'lexic', 'problem', 'cover', 'complet', 'concept', 'data', 'dictionari', 'mostli', 'concern', 'databas', 'scheme', 'design', 'rather', 'execut', 'oper', 'paper', 'introduc', 'view', 'lexic', 'subsystem', 'separ', 'compon', 'inform', 'system', 'architectur', 'deal', 'linguist', 'control', 'function', 'concern', 'lexic', 'problem', 'local', 'network', 'environ', 'lexic', 'suybsystem', 'special', 'effici', 'organ', 'program', 'packag', 'play', 'role', 'linguist', 'filter', 'broad', 'sens', 'lexic', 'incorrect', 'queri', 'promot', 'integr', 'databas', 'inform', 'retriev', 'system', 'facilit', 'creation', 'local', 'inform', 'system', 'hope', 'lexic', 'subsystem', 'becom', 'product', 'larg', 'especi', 'distribut', 'inform', 'system', 'inform', 'process', 'manag', 'vol', '16', 'februari', '1980', 'pp', '259267']\n",
            "Query tokens: ['relat', 'model', 'receiv', 'increas', 'attent', 'past', 'decad', 'advantag', 'includ', 'simplic', 'consist', 'sound', 'theoret', 'basi', 'articl', 'natur', 'view', 'inform', 'retriev', 'relat', 'demonstr', 'relat', 'model', 'present', 'relat', 'organ', 'bibliograph', 'databas', 'shown', 'notion', 'normal', 'introduc', 'first', 'second', 'third', 'fourth', 'normal', 'form', 'demonstr', 'relat', 'languag', 'discuss', 'includ', 'relat', 'calculu', 'relat', 'algebra', 'sequel', 'numer', 'exampl', 'pertin', 'inform', 'retriev', 'present', 'relat', 'languag', 'advantag', 'relat', 'approach', 'inform', 'retriev', 'note', 'jasi', 'vol', '32', 'januari', '1981', 'pp', '5164']\n",
            "Query tokens: ['techniqu', 'describ', 'automat', 'reformul', 'boolean', 'queri', 'base', 'patron', 'relev', 'judgement', 'initi', 'retriev', 'preval', 'measur', 'deriv', 'term', 'appear', 'retriev', 'set', 'document', 'reflect', 'term', 'distribut', 'among', 'relev', 'nonrelev', 'document', 'measur', 'use', 'guid', 'construct', 'boolean', 'queri', 'subsequ', 'retriev', 'illustr', 'techniqu', 'seri', 'test', 'describ', 'applic', 'small', 'data', 'base', 'experiment', 'environ', 'result', 'compar', 'favour', 'feedback', 'employ', 'smarttyp', 'system', 'extens', 'test', 'suggest', 'valid', 'techniqu', 'journal', 'document', 'vol', '36', 'septemb', '1980', 'pp', '197208']\n",
            "Query tokens: ['mani', 'inform', 'scientist', 'concern', 'oper', 'document', 'retriev', 'system', 'serv', 'scientist', 'variou', 'field', 'scientist', 'serv', 'system', 'often', 'member', 'call', 'invis', 'colleg', 'group', 'scientist', 'frequent', 'commun', 'one', 'anoth', 'involv', 'highli', 'special', 'subject', 'matter', 'often', 'group', 'consid', 'share', 'intellectu', 'perspect', 'regard', 'subject', 'matter', 'sometim', 'refer', 'paradigm', 'purpos', 'paper', 'show', 'possibl', 'identifi', 'paradigm', 'use', 'techniqu', 'citat', 'analysi', 'operation', 'notion', 'paradigm', 'consensu', 'structur', 'concept', 'field', 'suppos', 'obtain', 'set', 'paper', 'pertain', 'topic', 'alreadi', 'know', 'someth', 'field', 'read', 'text', 'mark', 'passag', 'certain', 'specif', 'concept', 'use', 'discuss', 'exampl', 'might', 'find', 'concept', 'design', 'appear', 'subset', 'paper', 'suppos', 'identifi', 'paper', 'concept', 'use', 'togeth', 'paper', 'certain', 'specifi', 'manner', 'clearli', 'concept', 'combin', 'natur', 'way', 'author', 'combin', 'concept', 'way', 'though', 'predomin', 'mode', 'may', 'emerg', 'set', 'concept', 'structur', 'given', 'total', 'admiss', 'combin', 'concept', 'taken', 'two', 'time', 'frequenc', 'given', 'combin', 'occur', 'sampl', 'paper', 'topic', 'measur', 'degre', 'consensu', 'regard', 'particular', 'concept', 'combin', 'within', 'corpu', 'concept', 'taken', 'two', 'time', 'structur', 'display', 'graph', 'concept', 'node', 'relat', 'repres', 'line', 'arc', 'connect', 'node', 'definit', 'concept', 'structur', 'similar', 'semant', 'network', 'artifici', 'intellig', 'except', 'approach', 'measur', 'consensu', 'weight', 'arc', 'graph', 'journal', 'document', 'vol', '36', 'septemb', '1980', 'pp', '183196']\n",
            "Query tokens: ['number', 'databas', 'record', 'contain', 'databas', 'onlin', 'use', 'databas', 'increas', 'dramat', 'past', 'sever', 'year', 'bring', '1979', 'total', 'bibliograph', 'bioliographicrel', 'natur', 'languag', 'databas', '528', '528', 'databas', 'contain', '148', 'million', 'record', 'million', 'onlin', 'search', 'conduct', 'via', 'major', 'us', 'canadian', 'system', '1979', 'bulletin', 'asi', 'vol', 'decemb', '1980', 'pp', '2729']\n",
            "Query tokens: ['major', 'defici', 'tradit', 'boolean', 'system', 'inabl', 'repres', 'vari', 'degre', 'document', 'may', 'written', 'subject', 'articl', 'isol', 'number', 'criteria', 'met', 'boolean', 'system', 'gener', 'weight', 'capabl', 'proven', 'one', 'weight', 'rule', 'satisfi', 'conditionsthat', 'associ', 'fuzzi', 'set', 'theoryand', 'weight', 'scheme', 'satisfi', 'properti', 'associ', 'boolean', 'algebra', 'well', 'probabilist', 'weight', 'introduc', 'altern', 'approach', 'two', 'system', 'compar', 'limit', 'zeroon', 'weight', 'system', 'consid', 'converg', 'tradit', 'boolean', 'retriev', 'jasi', 'vol', '32', 'juli', '1981']\n",
            "Query tokens: ['sever', 'paper', 'appear', 'analyz', 'recent', 'develop', 'problem', 'process', 'document', 'retriev', 'system', 'queri', 'express', 'boolean', 'express', 'purpos', 'paper', 'continu', 'analysi', 'shall', 'show', 'concept', 'threshold', 'valu', 'resolv', 'problem', 'inher', 'relev', 'weight', 'moreov', 'shall', 'explor', 'possibl', 'evalu', 'mechan', 'retriev', 'document', 'base', 'fuzzysettheoret', 'consider', 'inform', 'process', 'manag', 'vol', '17', '1981', 'pp', '127136']\n",
            "Query tokens: ['good', 'deal', 'work', 'inform', 'retriev', 'system', 'continu', 'weight', 'assign', 'index', 'term', 'describ', 'record', 'databas', 'andor', 'queri', 'term', 'describ', 'user', 'queri', 'recent', 'articl', 'analyz', 'retriev', 'system', 'continu', 'weight', 'either', 'type', 'andor', 'boolean', 'structur', 'queri', 'also', 'suggest', 'criteria', 'system', 'ought', 'satisfi', 'record', 'evalu', 'mechan', 'partial', 'satisfi', 'criteria', 'offer', 'care', 'analysi', 'base', 'gener', 'discret', 'weight', 'also', 'look', 'weight', 'entir', 'differ', 'approach', 'involv', 'threshold', 'gener', 'improv', 'evalu', 'mechan', 'seem', 'fulfil', 'larger', 'subset', 'desir', 'criteria', 'previou', 'mechan', 'new', 'mechan', 'allow', 'user', 'attach', 'threshold', 'queri', 'term', 'jasi', 'vol', '32', 'may', '1981', 'pp', '211216']\n",
            "Query tokens: ['onlin', 'retriev', 'system', 'may', 'difficult', 'use', 'especi', 'end', 'user', 'heterogen', 'complex', 'investig', 'concern', 'concept', 'translat', 'comput', 'interfac', 'mean', 'simplifi', 'access', 'oper', 'heterogen', 'bibliograph', 'retriev', 'system', 'databas', 'interfac', 'allow', 'user', 'make', 'request', 'common', 'languag', 'request', 'translat', 'interfac', 'appropri', 'command', 'whatev', 'system', 'interrog', 'system', 'respons', 'may', 'also', 'transform', 'interfac', 'common', 'form', 'given', 'user', 'thu', 'network', 'differ', 'system', 'made', 'look', 'like', 'singl', 'virtual', 'system', 'user', 'interfac', 'also', 'provid', 'instruct', 'search', 'aid', 'user', 'philosophi', 'design', 'implement', 'experiment', 'interfac', 'name', 'conit', 'describ', 'jasi', 'vol', '32', 'juli', '1981', 'pp', '287303']\n",
            "Query tokens: ['evalu', 'concept', 'translat', 'compuyt', 'interfac', 'simplifi', 'oper', 'multipl', 'heterogen', 'onlin', 'bibliograph', 'retriev', 'system', 'undertaken', 'experiment', 'retriev', 'system', 'name', 'conit', 'built', 'test', 'control', 'condit', 'inexperienc', 'end', 'user', 'detail', 'analysi', 'experiment', 'usag', 'show', 'user', 'abl', 'master', 'interfac', 'oper', 'suffici', 'well', 'find', 'relev', 'document', 'refer', 'success', 'attribut', 'part', 'simpl', 'command', 'languag', 'adequ', 'onlin', 'instruct', 'simplifi', 'naturallanguag', 'keywordstem', 'approach', 'search', 'conclud', 'oper', 'interfac', 'type', 'studi', 'provid', 'increas', 'usabl', 'exist', 'system', 'cost', 'effect', 'manner', 'especi', 'searcher', 'furthermor', 'advanc', 'interfac', 'base', 'improv', 'instruct', 'autom', 'search', 'strategi', 'techniqu', 'could', 'enhanc', 'retriev', 'effect', 'wide', 'class', 'user', 'jasi', 'vol', '32', 'juli', '1981', 'pp', '304317']\n",
            "Query tokens: ['paper', 'note', 'benefit', 'accru', 'interact', 'computer', 'retriev', 'system', 'micrograph', 'retriev', 'system', 'review', 'current', 'state', 'autom', 'micrograph', 'retriev', 'technolog', 'conclus', 'combin', 'advanc', 'commun', 'technolog', 'sophist', 'index', 'input', 'librari', 'inform', 'scientist', 'new', 'gener', 'autom', 'micrograph', 'devic', 'may', 'constitut', 'onlin', 'document', 'retriev', 'system', 'futur', 'journal', 'inform', 'scienc', '1980', 'pp', '345349']\n",
            "  Processed 100 queries...\n",
            "Query tokens: ['convent', 'inform', 'retriev', 'process', 'larg', 'base', 'data', 'movement', 'pointer', 'manipul', 'integ', 'arithmet', 'refin', 'retriev', 'algorithm', 'may', 'addit', 'benefit', 'substanti', 'comput', 'power', 'present', 'studi', 'number', 'parallel', 'process', 'method', 'describ', 'serv', 'enhanc', 'retriev', 'servic', 'convent', 'retriev', 'environ', 'parallel', 'list', 'process', 'parallel', 'search', 'facil', 'greatest', 'interest', 'advanc', 'system', 'use', 'array', 'processor', 'also', 'prove', 'benefici', 'variou', 'inform', 'retriev', 'process', 'examin', 'evid', 'given', 'demonstr', 'use', 'parallel', 'process', 'fast', 'comput', 'facil', 'inform', 'retriev', 'lectur', 'note', 'comput', 'scienc', 'iii', 'handler', 'ed', 'springer', 'verlag', 'berlinnew', 'york', '1981', 'pp', '328342']\n",
            "Query tokens: ['frequenc', 'characterist', 'term', 'document', 'collect', 'use', 'indic', 'term', 'import', 'content', 'analysi', 'index', 'purpos', 'particular', 'rare', 'frequent', 'term', 'normal', 'believ', 'less', 'effect', 'mediumfrequ', 'term', 'recent', 'automat', 'index', 'theori', 'devis', 'use', 'term', 'frequenc', 'characterist', 'also', 'relev', 'properti', 'term', 'major', 'termweight', 'theori', 'first', 'briefli', 'review', 'term', 'precis', 'term', 'util', 'weight', 'base', 'occurr', 'characterist', 'term', 'relev', 'oppos', 'nonrelev', 'document', 'collect', 'introduc', 'method', 'suggest', 'estim', 'relev', 'properti', 'term', 'base', 'overal', 'occurr', 'characterist', 'collect', 'final', 'experiment', 'evalu', 'result', 'shown', 'compar', 'weight', 'system', 'use', 'term', 'relev', 'properti', 'convent', 'frequencybas', 'methodolog', 'jasi', 'vol', '32', 'may', '1981', 'pp', '175186']\n",
            "Query tokens: ['paper', 'tackl', 'problem', 'one', 'might', 'select', 'search', 'term', 'use', 'relev', 'feedback', 'given', 'search', 'term', 'queri', 'search', 'term', 'extract', 'maximum', 'span', 'tree', 'connect', 'term', 'index', 'term', 'vocabulari', 'number', 'differ', 'span', 'tree', 'gener', 'varieti', 'associ', 'measur', 'retriev', 'effect', 'differ', 'span', 'tree', 'shown', 'approxim', 'effect', 'measur', 'term', 'precis', 'recal', 'retriev', 'test', 'done', 'three', 'differ', 'test', 'collect', 'inform', 'process', 'manag', 'vol', '17', '1981', 'pp', '7791']\n",
            "Query tokens: ['shown', 'map', 'particular', 'area', 'scienc', 'case', 'inform', 'scienc', 'done', 'use', 'author', 'unit', 'analysi', 'cocit', 'pair', 'author', 'variabl', 'indic', 'distanc', 'analysi', 'assum', 'two', 'author', 'cite', 'togeth', 'closer', 'relationship', 'raw', 'data', 'cocit', 'count', 'drawn', 'onlin', 'social', 'scisearch', 'social', 'scienc', 'citat', 'index', 'period', '19721979', 'gthe', 'result', 'map', 'show', 'identifi', 'author', 'group', 'akin', 'school', 'inform', 'scienc', 'locat', 'group', 'respect', 'degre', 'central', 'peripher', 'author', 'within', 'group', 'proxim', 'author', 'within', 'group', 'across', 'group', 'boundari', 'border', 'author', 'seem', 'connect', 'variou', 'area', 'research', 'posit', 'author', 'respect', 'map', 'axe', 'arbitrarili', 'set', 'span', 'diverg', 'group', 'order', 'aid', 'interpret', 'cocit', 'analysi', 'author', 'offer', 'new', 'techniqu', 'might', 'contribut', 'understand', 'intellectu', 'structur', 'scienc', 'possibl', 'area', 'extent', 'area', 'reli', 'serial', 'public', 'techniqu', 'establish', 'author', 'well', 'document', 'effect', 'unit', 'analyz', 'subject', 'specialti', 'jasi', 'vol', '32', 'may', '1981', 'pp', '163171']\n",
            "Query tokens: ['autom', 'document', 'cluster', 'procedur', 'describ', 'requir', 'use', 'interdocu', 'similar', 'matrix', 'independ', 'order', 'document', 'process', 'procedur', 'make', 'use', 'initi', 'set', 'cluster', 'deriv', 'certain', 'term', 'index', 'vocabulari', 'use', 'characteris', 'document', 'file', 'retriev', 'effect', 'obtain', 'use', 'cluster', 'file', 'compar', 'obtain', 'serial', 'search', 'use', 'singlelinkag', 'cluster', 'method', 'journal', 'inform', 'scienc', '1980', 'pp', '222231']\n",
            "\n",
            "============================================================\n",
            "📊 FINAL EVALUATION RESULTS\n",
            "============================================================\n",
            "Total Queries Evaluated: 76\n",
            "\n",
            "Precision@10: 0.2566 (25.66%)\n",
            "Recall@10: 0.1067 (10.67%)\n",
            "MAP@10: 0.0673 (6.73%)\n",
            "nDCG@10: 0.2830 (28.30%)\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}